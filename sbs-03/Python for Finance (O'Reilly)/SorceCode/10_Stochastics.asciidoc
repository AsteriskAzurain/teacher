[[stochastics]]


== Stochastics

[quote, Raheel Farooq]
____
[role="align_me_right"]
Predictability is not how things will go, but how they can go.
____

(((stochastic processes, importance of)))Nowadays, stochastics is one of the most important mathematical and numerical disciplines in finance. In the beginning of the modern era of finance, mainly in the 1970s and 1980s, the major goal of financial research was to come up with closed-form solutions for, e.g., option prices given a specific financial model. The requirements have drastically changed in recent years in that not only is the correct valuation of single financial instruments important to participants in the financial markets, but also the consistent valuation of whole derivatives books, for example. Similary, to come up with consistent risk measures across a whole financial institution, like value-at-risk and credit value adjustments, one needs to take into account the whole book of the institution and all its counterparties. Such daunting tasks can only be tackled by flexible and efficient numerical methods. Therefore, stochastics in general and Monte Carlo simulation in particular have risen to prominence.

This chapter introduces the following topics from a +Python+ perspective:

Random number generation:: 
    It all starts with (pseudo)random numbers, which build the basis for 
    all simulation efforts; although quasirandom numbers, e.g., based on 
    Sobol sequences, have gained some popularity in finance, pseudorandom 
    numbers still seem to be the benchmark.
Simulation:: 
    In finance, two simulation tasks are of particular importance:     
    simulation of _random variables_ and of _stochastic processes_.
Valuation:: 
    The two main disciplines when it comes to valuation are the valuation 
    of derivatives with _European exercise_ (at a specific date) and 
    _American exercise_ (over a specific time interval); there are also 
    instruments with _Bermudan exercise_, or exercise at a finite set of 
    specific dates.
Risk measures:: 
    Simulation lends itself pretty well to the calculation of 
    risk measures like value-at-risk, credit value-at-risk, and credit 
    value adjustments.


=== Random Numbers

((("random number generation", id="ix_rand", range="startofrange")))(((pseudorandom numbers)))(((NumPy, numpy.random sublibrary)))Throughout this chapter, to generate random numbersfootnote:[For simplicity, we will speak of _random numbers_ knowing that all numbers used will be _pseudorandom_.] we will work with the functions provided by the +numpy.random+ sublibrary:

// code cell start uuid: fba5b184-6652-4665-9053-1741d9b16bb9
[source, python]
----
In [1]: import numpy as np
        import numpy.random as npr
        import matplotlib.pyplot as plt
        %matplotlib inline
----

// code cell end

For example, the +rand+ function returns random numbers from the open interval [0,1) in the shape provided as a parameter to the function. The return object is an +ndarray+ object:

// code cell start uuid: 8763b99e-6b02-4003-8567-c0f505986e5a
[source, python]
----
In [2]: npr.rand(10)
----

----
Out[2]: array([ 0.40628966,  0.43098644,  0.9435419 ,  0.26760198,  0.2729951 ,
                0.67519064,  0.41349754,  0.3585647 ,  0.07450132,  0.95130158])
----

// code cell end

// code cell start uuid: 16f2a7c4-62dd-4d0f-bde9-fafb61e0fb64
[source, python]
----
In [3]: npr.rand(5, 5)
----

----
Out[3]: array([[ 0.87263851,  0.8143348 ,  0.34154499,  0.56695052,  0.60645041],
               [ 0.39398181,  0.71671577,  0.63568321,  0.61652708,  0.93526172],
               [ 0.12632038,  0.35793789,  0.04241014,  0.88085228,  0.54260211],
               [ 0.14503456,  0.32939077,  0.28834351,  0.4050322 ,  0.21120017],
               [ 0.45345805,  0.29771411,  0.67157606,  0.73563706,  0.48003387]
        ])
----

// code cell end

Such numbers can be easily transformed to cover other intervals of the real line. For instance, if you want to generate random numbers from the interval [__a__,__b__)=[5,10), you can transform the returned numbers from +rand+ as follows:

// code cell start uuid: 2d14b433-a7da-4aac-a534-56ab4c8a5d84
[source, python]
----
In [4]: a = 5.
        b = 10.
        npr.rand(10) * (b - a) + a
----

----
Out[4]: array([ 7.27123881,  6.51309437,  7.51380629,  7.84258434,  7.62199611,
                8.86229349,  6.78202851,  6.33248656,  8.10776244,  9.48668419])
----

// code cell end

This also works for multidimensional shapes due to +NumPy+ broadcasting:

// code cell start uuid: a05adb2b-5704-4189-b0e8-19318ac3f0b9
[source, python]
----
In [5]: npr.rand(5, 5) * (b - a) + a
----

----
Out[5]: array([[ 6.65649828,  6.51657569,  9.7912274 ,  8.93721206,  6.66937996],
               [ 8.97919481,  8.27547365,  5.00975386,  8.99797249,  6.05374605],
               [ 7.50268777,  8.43810167,  9.33608096,  8.5513646 ,  5.53651748],
               [ 7.04179874,  6.98111966,  8.42677435,  6.22325043,  6.39226557],
               [ 9.88334499,  7.59597546,  5.93724861,  5.39285822,  5.28435207]
        ])
----

// code cell end

(((random number generation, functions for simple)))(((simple random number generation)))<<simple_randoms>> lists functions for generating _simple_ random numbers.footnote:[Cf. http://docs.scipy.org/doc/numpy/reference/routines.random.html.]

[[simple_randoms]]
.Functions for simple random number generation
[options="header, unbreakable"]
|=======
|Function   |Parameters      |Description
|+rand+       |__++d0++__, __++d1++__, ..., __++dn++__ |Randoms in the given shape
|+randn+      |__++d0++__, __++d1++__, ..., __++dn++__ |A sample (or samples) from the standard normal distribution
|+randint+    |__++low++__[, __++high++__, __++size++__] |Random integers from _++low++_ (inclusive) to _++high++_ (exclusive)
|+random_integers+|__++low++__[, __++high++__, __++size++__] |Random integers between _++low++_ and __++high++__, inclusive
|+random_sample+|[__++size++__]        |Random floats in the half-open interval [0.0, 1.0)
|+random+     |[__++size++__]          |Random floats in the half-open interval [0.0, 1.0)
|+ranf+       |[__++size++__]          |Random floats in the half-open interval [0.0, 1.0)
|+sample+     |[__++size++__]          |Random floats in the half-open interval [0.0, 1.0)
|+choice+     |__++a++__[, __++size++__, __++replace++__, __++p++__] |Random sample from a given 1D array
|+bytes+      |__++length++__          |Random bytes
|=======

Let us visualize some random draws generated by selected functions from <<simple_randoms>>:

// code cell start uuid: 4618b170-6bd3-4500-905a-0fe402f198c1
[source, python]
----
In [6]: sample_size = 500
        rn1 = npr.rand(sample_size, 3)
        rn2 = npr.randint(0, 10, sample_size)
        rn3 = npr.sample(size=sample_size)
        a = [0, 25, 50, 75, 100]
        rn4 = npr.choice(a, size=sample_size)
----

// code cell end

<<rand_samples>> shows the results graphically for two continuous distributions and two discrete ones:

// code cell start uuid: d03c9514-c224-4d2b-ad2a-9285058823b0
[source, python]
----
In [7]: fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2,
                                                     figsize=(7, 7))
        ax1.hist(rn1, bins=25, stacked=True)
        ax1.set_title('rand')
        ax1.set_ylabel('frequency')
        ax1.grid(True)
        ax2.hist(rn2, bins=25)
        ax2.set_title('randint')
        ax2.grid(True)
        ax3.hist(rn3, bins=25)
        ax3.set_title('sample')
        ax3.set_ylabel('frequency')
        ax3.grid(True)
        ax4.hist(rn4, bins=25)
        ax4.set_title('choice')
        ax4.grid(True)
----

[[rand_samples]]
.Simple pseudorandom numbers
image::images/pyfi_1001.png[]

// code cell end

(((random number generation, functions according to distribution laws)))<<distributions>> lists functions for generating random numbers according to different _distributions_.footnote:[Cf. http://docs.scipy.org/doc/numpy/reference/routines.random.html.]

[[distributions]]
.Functions to generate random numbers according to different distribution laws
[options="header, unbreakable"]
|=======
|Function       |Parameters    | Description
|+beta+         |__++a++__, __++b++__[, __++size++__] | Samples for beta distribution over [0, 1]
|+binomial+     |__++n++__, __++p++__[, __++size++__] | Samples from a binomial distribution
|+chisquare+    |__++df++__[, __++size++__]   | Samples from a chi-square distribution
|+dirichlet+    |__++alpha++__[, __++size++__] | 	Samples from the Dirichlet distribution
|+exponential+  |[__++scale++__, __++size++__] | Samples from the exponential distribution
|+f+            |__++dfnum++__, __++dfden++__[, __++size++__] | Samples from an F distribution
|+gamma+        |__++shape++__[, __++scale++__, __++size++__] | Samples from a gamma distribution
|+geometric+    |__++p++__[, __++size++__]            | Samples from the geometric distribution
|+gumbel+       |[__++loc++__, __++scale++__, __++size++__]   | Samples from a Gumbel distribution
|+hypergeometric+ |__++ngood++__, __++nbad++__, __++nsample++__[, __++size++__]| Samples from a hypergeometric distribution
|+laplace+      |[__++loc++__, __++scale++__, __++size++__]   | Samples from the Laplace or double exponential distribution
|+logistic+     |[__++loc++__, __++scale++__, __++size++__]   | Samples from a logistic distribution
|+lognormalv+   |[__++mean++__, __++sigma++__, __++size++__]  | Samples from a log-normal distribution
|+logseries+    |__++p++__[, __++size++__]            | Samples from a logarithmic series distribution
|+multinomial+  |__++n++__, __++pvals++__[, __++size++__]     | Samples from a multinomial distribution
|+multivariate_normal+ |__++mean++__, __++cov++__[, __++size++__] | Samples from a multivariate normal distribution
|+negative_binomial+   |__++n++__, __++p++__[, __++size++__]      | Samples from a negative binomial distribution
|+noncentral_chisquare+|__++df++__, __++nonc++__[, __++size++__]  | Samples from a noncentral chi-square distribution
|+noncentral_f+        |__++dfnum++__, __++dfden++__, __++nonc++__[, __++size++__] | samples from the noncentral F distribution
|+normal+        |[__++loc++__, __++scale++__, __++size++__]      | Samples from a normal (Gaussian) distribution
|+pareto+        |__++a++__[, __++size++__]               | Samples from a Pareto II or Lomax distribution with specified shape
|+poisson+       |[__++lam++__, __++size++__]             | Samples from a Poisson distribution
|+power+         |__++a++__[, __++size++__]           | Samples in [0, 1] from a power distribution with positive exponent __++a++__–1
|+rayleigh+      |[__++scale++__, __++size++__]       | Samples from a Rayleigh distribution
|+standard_cauchy+      |[__++size++__]       | Samples from standard Cauchy distribution with mode = 0
|+standard_exponential+ |[__++size++__]       | Samples from the standard exponential distribution
|+standard_gamma+       |__++shape++__[, __++size++__]| Samples from a standard gamma distribution
|+standard_normal+      |[__++size++__]       | Samples from a standard normal distribution (mean=0, stdev=1)
|+standard_t+           |__++df++__[, __++size++__]   | Samples from a Student's t distribution with __++df++__ degrees of freedom
|+triangular+    |__++left++__, __++mode++__, __++right++__[, __++size++__] | Samples from the triangular distribution
|+uniform+      |[__++low++__, __++high++__, __++size++__]         | Samples from a uniform distribution
|+vonmises+      |__++mu++__, __++kappa++__[, __++size++__]         | Samples from a von Mises distribution
|+wald+          |__++mean++__, __++scale++__[, __++size++__]       | Samples from a Wald, or inverse Gaussian, distribution
|+weibull+       |__++a++__[, __++size++__]           | Samples from a Weibull distribution
|+zipf+          |__++a++__[, __++size++__]           | Samples from a Zipf distribution
|=======

Although there is much criticism around the use of (standard) normal distributions in finance, they are an indispensible tool and still the most widely used type of distribution, in analytical as well as numerical applications. One reason is that many financial models directly rest in one way or another on a normal distribution or a log-normal distribution. Another reason is that many financial models that do not rest directly on a (log-)normal assumption can be discretized, and therewith approximated for simulation purposes, by the use of the normal distribution.

As an illustration, we want to visualize random draws from the following distributions:

* **Standard normal** with mean of 0 and standard deviation of 1
* **Normal** with mean of 100 and standard deviation of 20
* **Chi square** with 0.5 degrees of freedom
* **Poisson** with lambda of 1

We do this as follows:

// code cell start uuid: fb2966ea-91ff-49c7-80e6-24bd6162cc5a
[source, python]
----
In [8]: sample_size = 500
        rn1 = npr.standard_normal(sample_size)
        rn2 = npr.normal(100, 20, sample_size)
        rn3 = npr.chisquare(df=0.5, size=sample_size)
        rn4 = npr.poisson(lam=1.0, size=sample_size)
----

// code cell end

(((Poisson distribution)))(((range="endofrange", startref="ix_rand")))<<rand_distris>> shows the results for the three continuous distributions and the discrete one (Poisson). The Poisson distribution is used, for example, to simulate the arrival of (rare) external events, like a jump in the price of an instrument or an exogenic shock. Here is the code that generates it:

// code cell start uuid: 3f790711-f965-4a10-b3df-47cc85d708d3
[source, python]
----
In [9]: fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, 
                                                     figsize=(7, 7))
        ax1.hist(rn1, bins=25)
        ax1.set_title('standard normal')
        ax1.set_ylabel('frequency')
        ax1.grid(True)
        ax2.hist(rn2, bins=25)
        ax2.set_title('normal(100, 20)')
        ax2.grid(True)
        ax3.hist(rn3, bins=25)
        ax3.set_title('chi square')
        ax3.set_ylabel('frequency')
        ax3.grid(True)
        ax4.hist(rn4, bins=25)
        ax4.set_title('Poisson')
        ax4.grid(True)
----

[[rand_distris]]
.Pseudorandom numbers from different distributions
image::images/pyfi_1002.png[]

// code cell end


=== Simulation

(((Monte Carlo simulation, importance of)))Monte Carlo simulation (MCS) is among the most important numerical techniques in finance, if not _the_ most important and widely used. This mainly stems from the fact that it is the most flexible numerical method when it comes to the evaluation of mathematical expressions (e.g., integrals), and specifically the valuation of financial derivatives. The flexibility comes at the cost of a relatively high computational burden, though, since often hundreds of thousands or even millions of complex computations have to be carried out to come up with a single value estimate.


==== Random Variables

(((simulation, random variables)))(((random variables)))(((Black-Scholes-Merton model, simulating future index level)))Consider, for example, the Black-Scholes-Merton setup for option pricing (cf. also <<introductory_examples>>). In their setup, the level of a stock index __S~T~__ at a future date __T__ given a level __S__~0~ as of today is given according to <<bsm_simple>>.

[[bsm_simple]]
[latexmath]
.Simulating future index level in Black-Scholes-Merton setup
++++
\begin{equation*}
S_T = S_0 \exp \left( \left( r- \frac{1}{2}\sigma^2\right) T + \sigma \sqrt{T} z \right)
\end{equation*}
++++

The variables and parameters have the following meaning:

__S~T~__::
	Index level at date __T__
__r__::
	Constant riskless short rate
&#x3c3;::
	Constant volatility (= standard deviation of returns) of __S__
__z__::
	Standard normally distributed random variable

This simple financial model is easily parameterized and simulated as follows:

// code cell start uuid: ac34499c-4675-457e-a0ac-40b8efcdb72e
[source, python]
----
In [10]: S0 = 100  # initial value
         r = 0.05  # constant short rate
         sigma = 0.25  # constant volatility
         T = 2.0  # in years
         I = 10000  # number of random draws
         ST1 = S0 * np.exp((r - 0.5 * sigma ** 2) * T
                      + sigma * np.sqrt(T) * npr.standard_normal(I))
----

// code cell end

((("simulation", "geometric Brownian motion", id="ix_Sgeo", range="startofrange")))The output of this simulation code is shown in <<gbm_T_sn>>:

// code cell start uuid: 7fc0b66a-9ce3-4c5e-bb99-d5e0363a6678
[source, python]
----
In [11]: plt.hist(ST1, bins=50)
         plt.xlabel('index level')
         plt.ylabel('frequency')
         plt.grid(True)
----

[[gbm_T_sn]]
.Simulated geometric Brownian motion (via standard_normal)
image::images/pyfi_1003.png[]

// code cell end

(((lognormal function)))<<gbm_T_sn>> suggests that the distribution of the random variable as defined in <<bsm_simple>> is _log-normal_. We could therefore also try to use the +lognormal+ function to directly derive the values for the random variable. In that case, we have to provide the mean and the standard deviation to the function:

// code cell start uuid: c37a0783-81b1-449f-924e-f792ba5017aa
[source, python]
----
In [12]: ST2 = S0 * npr.lognormal((r - 0.5 * sigma ** 2) * T,
                                 sigma * np.sqrt(T), size=I)
----

// code cell end

<<gbm_T_ln>> shows the output of the following simulation code:

// code cell start uuid: fea07d0c-7fc1-4ab8-8b21-fc36e73c3151
[source, python]
----
In [13]: plt.hist(ST2, bins=50)
         plt.xlabel('index level')
         plt.ylabel('frequency')
         plt.grid(True)
----

[[gbm_T_ln]]
.Simulated geometric Brownian motion (via lognormal)
image::images/pyfi_1004.png[]

By visual inspection, <<gbm_T_ln>> and <<gbm_T_sn>> indeed look pretty similar. But let us verify this more rigorously by comparing statistical moments of the resulting pass:[<phrase role='keep-together'>distributions.</phrase>]

(((SciPy, scipy.stats sublibrary)))(((print_statistics helper function)))(((range="endofrange", startref="ix_Sgeo")))To compare the distributional characteristics of simulation results we use the +scipy.stats+ sublibrary and the helper function +print_statistics+, as defined here:

// code cell start uuid: e5e17dcf-21f4-42ee-bcec-21103aaa8bb3
[source, python]
----
In [14]: import scipy.stats as scs
----

// code cell end

// code cell start uuid: d6f800c9-f38f-4fe1-8cb5-fe9253f1194c
[source, python]
----
In [15]: def print_statistics(a1, a2):
             ''' Prints selected statistics.
         
             Parameters
             ==========
             a1, a2 : ndarray objects
                 results object from simulation
             '''
             sta1 = scs.describe(a1)
             sta2 = scs.describe(a2)
             print "%14s %14s %14s" % \
                 ('statistic', 'data set 1', 'data set 2')
             print 45 * "-"
             print "%14s %14.3f %14.3f" % ('size', sta1[0], sta2[0])
             print "%14s %14.3f %14.3f" % ('min', sta1[1][0], sta2[1][0])
             print "%14s %14.3f %14.3f" % ('max', sta1[1][1], sta2[1][1])
             print "%14s %14.3f %14.3f" % ('mean', sta1[2], sta2[2])
             print "%14s %14.3f %14.3f" % ('std', np.sqrt(sta1[3]),
                                                  np.sqrt(sta2[3]))
             print "%14s %14.3f %14.3f" % ('skew', sta1[4], sta2[4])
             print "%14s %14.3f %14.3f" % ('kurtosis', sta1[5], sta2[5])
----

// code cell end

// code cell start uuid: 980679e8-56af-49e3-85f3-4b4d1ed90312
[source, python]
----
In [16]: print_statistics(ST1, ST2)
----

----
Out[16]:      statistic     data set 1     data set 2
         ---------------------------------------------
                   size      10000.000      10000.000
                    min         27.936         27.266
                    max        410.795        358.997
                   mean        110.442        110.528
                    std         39.932         40.894
                   skew          1.082          1.150
               kurtosis          1.927          2.273
         
----

// code cell end

(((simulation, sampling error in)))(((sampling error)))(((errors, sampling error)))(((discretization error)))(((errors, discretization error)))(((simulation, discretization error in)))Obviously, the statistics of both simulation results are quite similar. The differences are mainly due to what is called the _sampling error_ in simulation. Error can also be introduced when _discretely_ simulating _continuous_ stochastic processes—namely the _discretization error_, which plays no role here due to the static nature of the simulation pass:[<phrase role='keep-together'>approach.</phrase>]


==== Stochastic Processes

((("simulation", "stochastic processes", id="ix_Sstocp", range="startofrange")))((("stochastic processes", id="ix_stocp", range="startofrange")))(((stochastic processes, definition of)))(((Markov property)))(((memory-less processes)))Roughly speaking, a _stochastic process_ is a sequence of random variables. In that sense, we should expect something similar to a sequence of repeated simulations of a random variable when simulating a process. This is mainly true, apart from the fact that the draws are in general not independent but rather depend on the result(s) of the previous draw(s). In general, however, stochastic processes used in finance exhibit the _Markov property_, which mainly says that tomorrow's value of the process only depends on today's state of the process, and not any other more "historic" state or even the whole path history. The process then is also called _memoryless_.


===== Geometric Brownian motion

(((stochastic processes, geometric Brownian motion)))((("stochastic differential equation (SDE)")))Consider now the Black-Scholes-Merton model in its dynamic form, as described by the stochastic differential equation (SDE) in <<bsm_dynamic>>. Here, __Z~t~__ is a standard Brownian motion. The SDE is called a _geometric Brownian motion_. The values of __S~t~__ are log-normally distributed and the (marginal) returns latexmath:[$\frac{dS_t}{S_t}$] normally.

++++
<equation id="bsm_dynamic">
<title>Stochastic differential equation in Black-Scholes-Merton setup</title>
  <mathphrase>
    <emphasis>dS<subscript>t</subscript></emphasis> = <emphasis>rS<subscript>t</subscript>dt</emphasis> + &#x1d70e;<emphasis>S<subscript>t</subscript>dZ<subscript>t</subscript></emphasis>
  </mathphrase>
</equation>
++++

The SDE in <<bsm_dynamic>> can be discretized exactly by an Euler scheme. Such a scheme is presented in <<bsm_dyn_disc>>, with &#x1d6e5;__t__ being the fixed discretization interval and __z~t~__ being a standard normally distributed random variable.

[[bsm_dyn_disc]]
[latexmath]
.Simulating index levels dynamically in Black-Scholes-Merton setup
++++
\begin{equation*}
S_t = S_{t - \Delta t} \exp \left(\left(r- \frac{1}{2} \sigma^2 \right) \Delta t + \sigma \sqrt{\Delta t} z_t \right)
\end{equation*}
++++

As before, translation into +Python+ and +NumPy+ code is straightforward:

// code cell start uuid: a6b64214-0041-49cb-b7a8-7b4965d1d03a
[source, python]
----
In [17]: I = 10000
         M = 50
         dt = T / M
         S = np.zeros((M + 1, I))
         S[0] = S0
         for t in range(1, M + 1):
             S[t] = S[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt
                     + sigma * np.sqrt(dt) * npr.standard_normal(I))
----

// code cell end

The resulting end values for the index level are log-normally distributed again, as <<gbm_dt_hist>> illustrates:

// code cell start uuid: 969180df-b1f3-4f6d-8ec6-21cadbec06f1
[source, python]
----
In [18]: plt.hist(S[-1], bins=50)
         plt.xlabel('index level')
         plt.ylabel('frequency')
         plt.grid(True)
----

[[gbm_dt_hist]]
.Simulated geometric Brownian motion at maturity
image::images/pyfi_1005.png[]

// code cell end

The first four moments are also quite close to those resulting from the static simulation approach:

// code cell start uuid: 37d83fc1-6b2d-4d94-a5d1-75d2ba569283
[source, python]
----
In [19]: print_statistics(S[-1], ST2)
----

----
Out[19]:      statistic     data set 1     data set 2
         ---------------------------------------------
                   size      10000.000      10000.000
                    min         25.531         27.266
                    max        425.051        358.997
                   mean        110.900        110.528
                    std         40.135         40.894
                   skew          1.086          1.150
               kurtosis          2.224          2.273
         
----

// code cell end

<<gbm_dt_paths>> shows the first 10 simulated paths:

// code cell start uuid: c424f261-aa3f-4b04-9b5d-bb6824107fa0
[source, python]
----
In [20]: plt.plot(S[:, :10], lw=1.5)
         plt.xlabel('time')
         plt.ylabel('index level')
         plt.grid(True)
----
// code cell end

[[gbm_dt_paths]]
.Simulated geometric Brownian motion paths
image::images/pyfi_1006.png[]

Using the dynamic simulation approach not only allows us to visualize paths as displayed in <<gbm_dt_paths>>, but also to value options with American/Bermudan exercise or options whose payoff is path-dependent. You get the full dynamic picture, so to say:


===== Square-root diffusion

(((stochastic processes, square-root diffusion)))(((square-root diffusion)))(((Cox-Ingersoll-Ross SDE)))Another important class of financial processes is __mean-reverting processes__, which are used to model short rates or volatility processes, for example. A popular and widely used model is the _square-root diffusion_, as proposed by Cox, Ingersoll, and Ross (1985). <<srd_dynamic>> provides the respective SDE.

[[srd_dynamic]]
[latexmath]
.Stochastic differential equation for square-root diffusion
++++
\begin{equation*}
dx_t = \kappa(\theta - x_t) dt + \sigma \sqrt{x_t} dZ_t
\end{equation*}
++++

The variables and parameters have the following meaning:

__x~t~__::
	Process level at date __t__
__&#x3ba;__::
	Mean-reversion factor
__&#x3b8;__::
	Long-term mean of the process
&#x3c3;::
	Constant volatility parameter
__Z__::
	Standard Brownian motion

(((full truncation)))(((Euler scheme)))It is well known that the values of __x~t~__ are chi-squared distributed. However, as stated before, many financial models can be discretized and approximated by using the normal distribution (i.e., a so-called Euler discretization scheme). While the Euler scheme is exact for the geometric Brownian motion, it is biased for the majority of other stochastic processes. Even if there is an exact scheme available--one for the square-root diffusion will be presented shortly--the use of an Euler scheme might be desirable due to numerical and/or computational reasons. Defining __s__ &#x2261; __t__ – &#x394;__t__ and __x^+^__ &#x2261; max(__x__,0), <<srd_disc_1>> presents such an Euler scheme. This particular one is generally called _full truncation_ in the literature (cf. Hilpisch (2015)).

[[srd_disc_1]]
[latexmath]
.Euler discretization for square-root diffusion
++++
\begin{eqnarray*}
\tilde{x}_t &=& \tilde{x}_{s} + \kappa(\theta - \tilde{x}_{s}^+) \Delta t + \sigma \sqrt{\tilde{x}_{s}^+} \sqrt{\Delta t} z_t \\
x_t &=& \tilde{x}_t^+
\end{eqnarray*}
++++

We parameterize the model for the simulations to follow with values that could represent those of a short rate model:

// code cell start uuid: b00481e7-074a-4d04-a65d-4ee95f971116
[source, python]
----
In [21]: x0 = 0.05
         kappa = 3.0
         theta = 0.02
         sigma = 0.1
----

// code cell end

The square-root diffusion has the convenient and realistic characteristic that the values of __x~t~__ remain strictly positive. When discretizing it by an Euler scheme, negative values cannot be excluded. That is the reason why one works always with the positive version of the originally simulated process. In the simulation code, one therefore needs two +ndarray+ objects instead of only one:

// code cell start uuid: e085f53a-d065-424e-b1f4-d41c64464c2a
[source, python]
----
In [22]: I = 10000
         M = 50
         dt = T / M
         def srd_euler():
             xh = np.zeros((M + 1, I))
             x1 = np.zeros_like(xh)
             xh[0] = x0
             x1[0] = x0
             for t in range(1, M + 1):
                 xh[t] = (xh[t - 1]
                       + kappa * (theta - np.maximum(xh[t - 1], 0)) * dt
                       + sigma * np.sqrt(np.maximum(xh[t - 1], 0)) * np.sqrt(dt)
                       * npr.standard_normal(I))
             x1 = np.maximum(xh, 0)
             return x1
         x1 = srd_euler()
----

// code cell end

<<srd_hist_Euler>> shows the result of the simulation graphically as a histogram:

// code cell start uuid: 93283652-414e-4773-99ca-00e0b24cc088
[source, python]
----
In [23]: plt.hist(x1[-1], bins=50)
         plt.xlabel('value')
         plt.ylabel('frequency')
         plt.grid(True)
----

[[srd_hist_Euler]]
.Simulated square-root diffusion at maturity (Euler scheme)
image::images/pyfi_1007.png[]

// code cell end

<<srd_dt_Euler>> then shows the first 10 simulated paths, illustrating the resulting negative, averagef drift (due to __x__~0~ > &#x1d703;) and the convergence to &#x1d703; = 0.02:

// code cell start uuid: 59c2b6b1-7c7d-44bd-8ae3-8ad16dd2eb30
[source, python]
----
In [24]: plt.plot(x1[:, :10], lw=1.5)
         plt.xlabel('time')
         plt.ylabel('index level')
         plt.grid(True)
----

[[srd_dt_Euler]]
.Simulated square-root diffusion paths (Euler scheme)
image::images/pyfi_1008.png[]

// code cell end

Let us now get more exact. <<srd_disc_2>> presents the exact discretization scheme for the square-root diffusion based on the noncentral chi-square distribution latexmath:[$\chi '^{2}_{d}$] with latexmath:[$df=\frac{4\theta\kappa}{\sigma^{2}}$] degrees of freedom and noncentrality parameter latexmath:[$nc=\frac{4\kappa e^{-\kappa \Delta t}}{\sigma^{2}(1-e^{-\kappa\Delta t})}x_{s}$].

[[srd_disc_2]]
[latexmath]
.Exact discretization for square-root diffusion
++++
\begin{equation*}
x_t=\frac{\sigma^{2}\left(1-e^{-\kappa\Delta t}\right)}{4\kappa} \chi '^{2}_{d}\left(\frac{4\kappa e^{-\kappa \Delta t}}{\sigma^{2}(1-e^{-\kappa\Delta t})}x_{s}\right)
\end{equation*}
++++

The +Python+ implementation of this discretization scheme is a bit more involved but still quite concise:

// code cell start uuid: b901c93e-a4a9-4f8f-98d9-69754cb586bf
[source, python]
----
In [25]: def srd_exact():
             x2 = np.zeros((M + 1, I))
             x2[0] = x0
             for t in range(1, M + 1):
                 df = 4 * theta * kappa / sigma ** 2
                 c = (sigma ** 2 * (1 - np.exp(-kappa * dt))) / (4 * kappa)
                 nc = np.exp(-kappa * dt) / c * x2[t - 1]
                 x2[t] = c * npr.noncentral_chisquare(df, nc, size=I)
             return x2
         x2 = srd_exact()
----

// code cell end

<<srd_hist_exact>> shows the output of the simulation with the exact scheme as a histogram:

// code cell start uuid: 98648791-2251-4313-baef-e65e4f3ea059
[source, python]
----
In [26]: plt.hist(x2[-1], bins=50)
         plt.xlabel('value')
         plt.ylabel('frequency')
         plt.grid(True)
----

[[srd_hist_exact]]
.Simulated square-root diffusion at maturity (exact scheme)
image::images/pyfi_1009.png[]

// code cell end

<<srd_dt_exact>> presents as before the first 10 simulated paths, again displaying the negative average drift and the convergence to &#x1d703;:

// code cell start uuid: 3d998e1a-e225-4de8-b09b-abf8651d30cb
[source, python]
----
In [27]: plt.plot(x2[:, :10], lw=1.5)
         plt.xlabel('time')
         plt.ylabel('index level')
         plt.grid(True)
----

[[srd_dt_exact]]
.Simulated square-root diffusion paths (exact scheme)
image::images/pyfi_1010.png[]

// code cell end

Comparing the main statistics from the different approaches reveals that the biased Euler scheme indeed performs quite well when it comes to the desired statistical properties:

// code cell start uuid: fc247695-7a20-4452-8c74-96ace26f2ebe
[source, python]
----
In [28]: print_statistics(x1[-1], x2[-1])
----

----
Out[28]:      statistic     data set 1     data set 2
         ---------------------------------------------
                   size      10000.000      10000.000
                    min          0.004          0.005
                    max          0.049          0.050
                   mean          0.020          0.020
                    std          0.006          0.006
                   skew          0.529          0.572
               kurtosis          0.418          0.503
         
----

// code cell end

However, a major difference can be observed in terms of execution speed, since sampling from the noncentral chi-square distribution is more computationally demanding than from the standard normal distribution. To illustrate this point, consider a larger number of paths to be simulated:

// code cell start uuid: 7f49cc7d-5264-459c-a9b7-d602daed9f2b
[source, python]
----
In [29]: I = 250000
         %time x1 = srd_euler()
----

----
Out[29]: CPU times: user 1.02 s, sys: 84 ms, total: 1.11 s
         Wall time: 1.11 s
         
----

// code cell end

// code cell start uuid: ede482c4-ec2c-43e2-8128-0c97b44469bd
[source, python]
----
In [30]: %time x2 = srd_exact()
----

----
Out[30]: CPU times: user 2.26 s, sys: 32 ms, total: 2.3 s
         Wall time: 2.3 s
         
----

// code cell end

The exact scheme takes roughly twice as much time for virtually the same results as with the Euler scheme:

// code cell start uuid: 84a26be5-eede-4478-9f67-c6a97f9804f9
[source, python]
----
In [31]: print_statistics(x1[-1], x2[-1])
         x1 = 0.0; x2 = 0.0
----

----
Out[31]:      statistic     data set 1     data set 2
         ---------------------------------------------
                   size     250000.000     250000.000
                    min          0.003          0.004
                    max          0.069          0.060
                   mean          0.020          0.020
                    std          0.006          0.006
                   skew          0.554          0.578
               kurtosis          0.488          0.502
         
----

// code cell end


===== Stochastic volatility

(((stochastic processes, stochastic volatility model)))((("volatility, stochastic model")))(((Heston stochastic volatility model)))One of the major simplifying assumptions of the Black-Scholes-Merton model is the _constant_ volatility. However, volatility in general is neither constant nor deterministic; it is _stochastic_. Therefore, a major advancement with regard to financial modeling was achieved in the early 1990s with the introduction of so-called _stochastic volatility models_. One of the most popular models that fall into that category is that of Heston (1993), which is presented in <<stoch_vol_sde>>.

[[stoch_vol_sde]]
[latexmath]
.Stochastic differential equations for Heston stochastic volatility model
++++
\begin{eqnarray*}
dS_{t}&=&rS_{t}dt+\sqrt{v_{t}}S_{t}dZ_{t}^{1} \\
dv_{t}&=&\kappa_{v} (\theta_{v} -v_{t})dt+\sigma_{v} \sqrt{v_{t}}dZ_{t}^{2} \\
dZ_{t}^{1}dZ_{t}^{2}&=&\rho
\end{eqnarray*}
++++

(((leverage effect)))The meaning of the single variables and parameters can now be inferred easily from the discussion of the geometric Brownian motion and the square-root diffusion. The parameter &#x1d70c; represents the instantaneous correlation between the two standard Brownian motions latexmath:[$Z^1_t, Z^2_t$]. This allows us to account for a stylized fact called the _leverage effect_, which in essence states that volatility goes up in times of stress (declining markets) and goes down in times of a bull market (rising markets).

Consider the following parameterization of the model:

// code cell start uuid: 786bc4c9-bff7-4a6d-9ae5-1f62c1813518
[source, python]
----
In [32]: S0 = 100.
         r = 0.05
         v0 = 0.1
         kappa = 3.0
         theta = 0.25
         sigma = 0.1
         rho = 0.6
         T = 1.0
----

// code cell end

To account for the correlation between the two stochastic processes, we need to determine the Cholesky decomposition of the correlation matrix:

// code cell start uuid: 0db5ac22-1065-4fd5-92a8-3ccb0780d34c
[source, python]
----
In [33]: corr_mat = np.zeros((2, 2))
         corr_mat[0, :] = [1.0, rho]
         corr_mat[1, :] = [rho, 1.0]
         cho_mat = np.linalg.cholesky(corr_mat)
----

// code cell end

// code cell start uuid: 41b7d810-38b5-4831-bb66-84a57c97415b
[source, python]
----
In [34]: cho_mat
----

----
Out[34]: array([[ 1. ,  0. ],
                [ 0.6,  0.8]])
----

// code cell end

Before we start simulating the stochastic processes, we generate the whole set of random numbers for both processes, looking to use set 0 for the index process and set 1 for the volatility process:

// code cell start uuid: b16ca288-23eb-463b-9b63-4765eea564f9
[source, python]
----
In [35]: M = 50
         I = 10000
         ran_num = npr.standard_normal((2, M + 1, I))
----

// code cell end

For the volatility process modeled by the square-root diffusion process type, we use the Euler scheme, taking into account the correlation parameter:

// code cell start uuid: e7ae274e-fec0-43f5-a171-0dd5f131e6c2
[source, python]
----
In [36]: dt = T / M
         v = np.zeros_like(ran_num[0])
         vh = np.zeros_like(v)
         v[0] = v0
         vh[0] = v0
         for t in range(1, M + 1):
             ran = np.dot(cho_mat, ran_num[:, t, :])
             vh[t] = (vh[t - 1] + kappa * (theta - np.maximum(vh[t - 1], 0)) * dt
                   + sigma * np.sqrt(np.maximum(vh[t - 1], 0)) * np.sqrt(dt)
                   * ran[1])
         v = np.maximum(vh, 0)
----

// code cell end

For the index level process, we also take into account the correlation and use the exact Euler scheme for the geometric Brownian motion:

// code cell start uuid: 0016d6a1-4c5c-4617-847a-d0d1510c3fb9
[source, python]
----
In [37]: S = np.zeros_like(ran_num[0])
         S[0] = S0
         for t in range(1, M + 1):
             ran = np.dot(cho_mat, ran_num[:, t, :])
             S[t] = S[t - 1] * np.exp((r - 0.5 * v[t]) * dt +
                             np.sqrt(v[t]) * ran[0] * np.sqrt(dt))
----

// code cell end

This illustrates another advantage of working with the Euler scheme for the square-root diffusion: _correlation is easily and consistently accounted for_ since we only draw standard normally distributed random numbers. There is no simple way of achieving the same with a mixed approach, using Euler for the index and the noncentral chi square-based exact approach for the volatility process.

<<sv_hist>> shows the simulation results as a histogram for both the index level process and the volatility process:

// code cell start uuid: 5db99fd6-5e32-4c1f-8186-fe6ac910b0c8
[source, python]
----
In [38]: fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))
         ax1.hist(S[-1], bins=50)
         ax1.set_xlabel('index level')
         ax1.set_ylabel('frequency')
         ax1.grid(True)
         ax2.hist(v[-1], bins=50)
         ax2.set_xlabel('volatility')
         ax2.grid(True)
----

// code cell end

An inspection of the first 10 simulated paths of each process (cf. <<sv_paths>>) shows that the volatility process is drifting positively on average and that it, as expected, converges to &#x1d703;~__v__~ = 0.25:

// code cell start uuid: 0b542695-d86d-47d9-8be4-760cd9a7786b
[source, python]
----
In [39]: fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(7, 6))
         ax1.plot(S[:, :10], lw=1.5)
         ax1.set_ylabel('index level')
         ax1.grid(True)
         ax2.plot(v[:, :10], lw=1.5)
         ax2.set_xlabel('time')
         ax2.set_ylabel('volatility')
         ax2.grid(True)
----

[[sv_hist]]
.Simulated stochastic volatility model at maturity
image::images/pyfi_1011.png[]

[[sv_paths]]
.Simulated stochastic volatility model paths
image::images/pyfi_1012.png[]

// code cell end

Finally, let us take a brief look at the statistics for the last point in time for both data sets, showing a pretty high maximum value for the index level process. In fact, this is much higher than a geometric Brownian motion with constant volatility could ever climb, __ceteris paribus__:

// code cell start uuid: 398e803e-e0d8-4bc1-9c2a-53ad78cf524d
[source, python]
----
In [40]: print_statistics(S[-1], v[-1])
----

----
Out[40]:      statistic     data set 1     data set 2
         ---------------------------------------------
                   size      10000.000      10000.000
                    min         19.814          0.174
                    max        600.080          0.322
                   mean        108.818          0.243
                    std         52.535          0.020
                   skew          1.702          0.151
               kurtosis          5.407          0.071
         
----

// code cell end


===== Jump diffusion

(((stochastic processes, jump diffusion)))(((jump diffusion)))Stochastic volatility and the leverage effect are stylized (empirical) facts found in a number of markets. Another important stylized empirical fact is the existence of _jumps_ in asset prices and, for example, volatility. In 1976, Merton published his jump diffusion model, enhancing the Black-Scholes-Merton setup by a model component generating jumps with log-normal distribution. The risk-neutral SDE is presented in <<jd_sde>>.

++++
<equation id="jd_sde">
<title>Stochastic differential equation for Merton jump diffusion model</title>
  <mathphrase>
    <emphasis>dS<subscript>t</subscript></emphasis> = (<emphasis>r</emphasis> – <emphasis>r<subscript>J</subscript></emphasis>)<emphasis>S<subscript>t</subscript>dt</emphasis> + &#x1d70e;<emphasis>S<subscript>t</subscript>dZ<subscript>t</subscript></emphasis> + <emphasis>J<subscript>t</subscript>S<subscript>t</subscript>dN<subscript>t</subscript></emphasis>
  </mathphrase>
</equation>
++++

For completeness, here is an overview of the variables' and parameters' meaning:

__S~t~__::
	Index level at date __t__
__r__::
	Constant riskless short rate
latexmath:[$r_{J}\equiv \lambda \cdot \left(e^{\mu_{J}+\delta^{2}/2}-1\right)$]::
	Drift correction for jump to maintain risk neutrality
&#x3c3;::
	Constant volatility of __S__
__Z~t~__::
	Standard Brownian motion
__J~t~__::
	Jump at date __t__ with distribution ...
** ... latexmath:[$\log(1+J_{t})\approx \mathbf{N}\left(\log(1+\mu_{J})-\frac{\delta^{2}}{2},\delta^{2}\right)$] with ...
** ... **N** as the cumulative distribution function of a standard normal random variable
__N~t~__::
  Poisson process with intensity &#x1d706;

<<jd_disc>> presents an Euler discretization for the jump diffusion where the latexmath:[$z^{n}_{t}$] are standard normally distributed and the latexmath:[$y_{t}$] are Poisson distributed with intensity &#x1d706;.

[[jd_disc]]
[latexmath]
.Euler discretization for Merton jump diffusion model
++++
\begin{equation*}
S_{t}=S_{t-\Delta t}\left(e^{(r-r_{J}-\sigma^{2}/2)\Delta t+\sigma \sqrt{\Delta t}z^{1}_{t}}+ \left(e^{\mu_{J}+\delta z^{2}_{t}}-1\right)y_{t}\right)
\end{equation*}
++++

Given the discretization scheme, consider the following numerical parameterization:

// code cell start uuid: 4d34dbf3-196e-4125-a11d-f967982540e2
[source, python]
----
In [41]: S0 = 100.
         r = 0.05
         sigma = 0.2
         lamb = 0.75
         mu = -0.6
         delta = 0.25
         T = 1.0
----

// code cell end

To simulate the jump diffusion, we need to generate three sets of (independent) random numbers:

// code cell start uuid: b22527e8-afc1-4c69-8253-4e8b6a64f0da
[source, python]
----
In [42]: M = 50
         I = 10000
         dt = T / M
         rj = lamb * (np.exp(mu + 0.5 * delta ** 2) - 1)
         S = np.zeros((M + 1, I))
         S[0] = S0
         sn1 = npr.standard_normal((M + 1, I))
         sn2 = npr.standard_normal((M + 1, I))
         poi = npr.poisson(lamb * dt, (M + 1, I))
         for t in range(1, M + 1, 1):
             S[t] = S[t - 1] * (np.exp((r - rj - 0.5 * sigma ** 2) * dt
                                + sigma * np.sqrt(dt) * sn1[t])
                                + (np.exp(mu + delta * sn2[t]) - 1)
                                * poi[t])
             S[t] = np.maximum(S[t], 0)
----

// code cell end

Since we have assumed a highly negative mean for the jump, it should not come as a surprise that the final values of the simulated index level are more _right-skewed_ in <<jd_hist>> compared to a typical log-normal distribution:

// code cell start uuid: 19508067-6759-4e88-9276-0d21a0be9e8e
[source, python]
----
In [43]: plt.hist(S[-1], bins=50)
         plt.xlabel('value')
         plt.ylabel('frequency')
         plt.grid(True)
----

[[jd_hist]]
.Simulated jump diffusion at maturity
image::images/pyfi_1013.png[]

// code cell end

The highly negative jumps can also be found in the first 10 simulated index level paths, as presented in <<jd_paths>>:

// code cell start uuid: 27046a97-3c3c-4265-bde7-45f9b71dc001
[source, python]
----
In [44]: plt.plot(S[:, :10], lw=1.5)
         plt.xlabel('time')
         plt.ylabel('index level')
         plt.grid(True)
----

[[jd_paths]]
.Simulated jump diffusion paths
image::images/pyfi_1014.png[]

// code cell end


==== Variance Reduction

(((simulation, variance reduction)))(((variance reduction)))(((pseudorandom numbers)))Not only because of the fact that the +Python+ functions we have used so far generate _pseudorandom_ numbers, but also due to the varying sizes of the samples drawn, resulting sets of numbers might not exhibit statistics really close enough to the expected/desired ones. For example, you would expect a set of standard normally distributed random numbers to show a mean of 0 and a standard deviation of 1. Let us check what statistics different sets of random numbers exhibit. To achieve a realistic comparison, we fix the seed value for the random number generator:

// code cell start uuid: 293a9f5c-7ae1-4994-b11d-64ba5312559a
[source, python]
----
In [45]: print "%15s %15s" % ('Mean', 'Std. Deviation')
         print 31 * "-"
         for i in range(1, 31, 2):
             npr.seed(1000)
             sn = npr.standard_normal(i ** 2 * 10000)
             print "%15.12f %15.12f" % (sn.mean(), sn.std())
----

----
Out[45]:            Mean  Std. Deviation
         -------------------------------
         -0.011870394558  1.008752430725
         -0.002815667298  1.002729536352
         -0.003847776704  1.000594044165
         -0.003058113374  1.001086345326
         -0.001685126538  1.001630849589
         -0.001175212007  1.001347684642
         -0.000803969036  1.000159081432
         -0.000601970954  0.999506522127
         -0.000147787693  0.999571756099
         -0.000313035581  0.999646153704
         -0.000178447061  0.999677277878
          0.000096501709  0.999684346792
         -0.000135677013  0.999823841902
         -0.000015726986  0.999906493379
         -0.000039368519  1.000063091949
----

// code cell end

// code cell start uuid: 5940d5f7-72ed-4fd2-8a48-d66c2d5e45db
[source, python]
----
In [46]: i ** 2 * 10000
----

----
Out[46]: 8410000
----

// code cell end

The results show that the statistics "somehow" get better the larger the number of draws becomes. But they still do not match the desired ones, even in our largest sample with more than 8,000,000 random numbers.

(((antithetic variates)))Fortunately, there are easy-to-implement, generic variance reduction techniques available to improve the matching of the first two moments of the (standard) normal distribution. The first technique is to use _antithetic variates_. This approach simply draws only half the desired number of random draws, and adds the same set of random numbers with the opposite sign afterward.footnote:[The described method works for symmetric median 0 random variables only, like standard normally distributed random variables, which we almost exclusively use throughout.] For example, if the random number generator (i.e., the respective +Python+ function) draws 0.5, then another number with value –0.5 is added to the set.

(((concatenate function)))(((NumPy, concatenate function)))With +NumPy+ this is concisely implemented by using the function +concatenate+:

// code cell start uuid: 732f2ba4-3133-4508-92a1-10ee47519f36
[source, python]
----
In [47]: sn = npr.standard_normal(10000 / 2)
         sn = np.concatenate((sn, -sn))
         np.shape(sn)
----

----
Out[47]: (10000,)
----

// code cell end

The following repeats the exercise from before, this time using antithetic variates:

// code cell start uuid: 3f166fbb-ed57-403f-b251-1b5579ec261d
[source, python]
----
In [48]: print "%15s %15s" % ('Mean', 'Std. Deviation')
         print 31 * "-"
         for i in range(1, 31, 2):
             npr.seed(1000)
             sn = npr.standard_normal(i ** 2 * 10000 / 2)
             sn = np.concatenate((sn, -sn))
             print "%15.12f %15.12f" % (sn.mean(), sn.std())
----

----
Out[48]:            Mean  Std. Deviation
         -------------------------------
          0.000000000000  1.009653753942
         -0.000000000000  1.000413716783
          0.000000000000  1.002925061201
         -0.000000000000  1.000755212673
          0.000000000000  1.001636910076
         -0.000000000000  1.000726758438
         -0.000000000000  1.001621265149
          0.000000000000  1.001203722778
         -0.000000000000  1.000556669784
          0.000000000000  1.000113464185
         -0.000000000000  0.999435175324
          0.000000000000  0.999356961431
         -0.000000000000  0.999641436845
         -0.000000000000  0.999642768905
         -0.000000000000  0.999638303451
----

// code cell end

As you immediately notice, this approach corrects the first moment perfectly—which should not come as a surprise. This follows from the fact that whenever a number __n__ is drawn, __–n__ is also added. Since we only have such pairs, the mean is equal to 0 over the whole set of random numbers. However, this approach does not have any influence on the second moment, the standard deviation.

(((moment matching)))Using another variance reduction technique, called _moment matching_, helps correct in one step both the first and second moments:

// code cell start uuid: de17794f-4dfd-4441-8d0f-bd097ac0da2c
[source, python]
----
In [49]: sn = npr.standard_normal(10000)
----

// code cell end

// code cell start uuid: 0251bf81-b4d8-4828-80be-9ff972204d06
[source, python]
----
In [50]: sn.mean()
----

----
Out[50]: -0.001165998295162494
----

// code cell end

// code cell start uuid: a59c5234-0398-4260-9bcb-d63cd6a7c917
[source, python]
----
In [51]: sn.std()
----

----
Out[51]: 0.99125592020460496
----

// code cell end

By subtracting the mean from every single random number and dividing every single number by the standard deviation, we get a set of random numbers matching the desired first and second moments of the standard normal distribution (almost) perfectly:

// code cell start uuid: 699ea494-9c78-4ddc-b153-ce291039f77e
[source, python]
----
In [52]: sn_new = (sn - sn.mean()) / sn.std()
----

// code cell end

// code cell start uuid: e5836915-236c-4c1b-9012-20fb52e50608
[source, python]
----
In [53]: sn_new.mean()
----

----
Out[53]: -2.3803181647963357e-17
----

// code cell end

// code cell start uuid: 5113ce74-07a2-4b16-b8d0-7ed9495ccb9b
[source, python]
----
In [54]: sn_new.std()
----

----
Out[54]: 0.99999999999999989
----

// code cell end

(((range="endofrange", startref="ix_Sstocp")))(((range="endofrange", startref="ix_stocp")))The following function utilizes the insight with regard to variance reduction techniques and generates standard normal random numbers for process simulation using either two, one, or no variance reduction technique(s):

// code cell start uuid: f566cd19-61d3-4c69-9391-cb1c906d23c3
[source, python]
----
In [55]: def gen_sn(M, I, anti_paths=True, mo_match=True):
             ''' Function to generate random numbers for simulation.
         
             Parameters
             ==========
             M : int
                 number of time intervals for discretization
             I : int
                 number of paths to be simulated
             anti_paths: Boolean
                 use of antithetic variates
             mo_math : Boolean
                 use of moment matching
             '''
             if anti_paths is True:
                 sn = npr.standard_normal((M + 1, I / 2))
                 sn = np.concatenate((sn, -sn), axis=1)
             else:
                 sn = npr.standard_normal((M + 1, I))
             if mo_match is True:
                 sn = (sn - sn.mean()) / sn.std()
             return sn
----

// code cell end


=== Valuation

((("valuation of contingent claims", id="ix_valu", range="startofrange")))((("Monte Carlo simulation", "valuation of contingent claims", id="ix_MCvalu", range="startofrange")))((("contingent claims, valuation of", id="ix_cont", range="startofrange")))(((Fundamental Theorem of Asset Pricing)))One of the most important applications of Monte Carlo simulation is the _valuation of contingent claims_ (options, derivatives, hybrid instruments, etc.). Simply stated, in a risk-neutral world, the value of a contingent claim is the discounted expected payoff under the risk-neutral (martingale) measure. This is the probability measure that makes all risk factors (stocks, indices, etc.) drift at the riskless short rate. According to the Fundamental Theorem of Asset Pricing, the existence of such a probability measure is equivalent to the absence of arbitrage.

(((call options, definition of)))((("put options, definition of")))(((European options, definition of)))(((American options, definition of)))A financial option embodies the right to buy (_call option_) or sell (_put option_) a specified financial instrument at a given (maturity) date (_European option_), or over a specified period of time (_American option_), at a given price (the so-called __strike price__). Let us first consider the much simpler case of European options in terms of valuation.


==== European Options

(((valuation of contingent claims, European options)))(((European options, valuation of contingent claims)))((("contingent claims, valuation of", "European options")))The payoff of a European call option on an index at maturity is given by __h__(__S~T~__) &#x2261; max(__S~T~__ – __K__,0), where __S~T~__ is the index level at maturity date __T__ and __K__ is the strike price. Given a, or in complete markets _the_, risk-neutral measure for the relevant stochastic process (e.g., geometric Brownian motion), the price of such an option is given by the formula in <<rn_pricing>>.

[[rn_pricing]]
[latexmath]
.Pricing by risk-neutral expectation
++++
\begin{equation*}
C_0 = e^{-r T}\mathbf{E}^Q_0(h(S_T)) =  e^{-r T} \int_{0}^{\infty}h(s) q(s) ds
\end{equation*}
++++

<<math_tools>> briefly sketches how to numerically evaluate an integral by Monte Carlo simulation. This approach is used in the following and applied to <<rn_pricing>>. <<rn_estimator>> provides the respective Monte Carlo estimator for the European option, where latexmath:[$\tilde{S}_T^i$] is the __i__th simulated index level at maturity.

[[rn_estimator]]
[latexmath]
.Risk-neutral Monte Carlo estimator
++++
\begin{equation*}
\tilde{C_0} = e^{-r T}\frac{1}{I} \sum_{i=1}^{I} h(\tilde{S}_T^i)
\end{equation*}
++++

Consider now the following parameterization for the geometric Brownian motion and the valuation function +gbm_mcs_stat+, taking as a parameter only the strike price. Here, only the index level at maturity is simulated:

// code cell start uuid: 693f44be-b3dd-4820-9610-a127f0e9b31b
[source, python]
----
In [56]: S0 = 100.
         r = 0.05
         sigma = 0.25
         T = 1.0
         I = 50000
         def gbm_mcs_stat(K):
             ''' Valuation of European call option in Black-Scholes-Merton
             by Monte Carlo simulation (of index level at maturity)
         
             Parameters
             ==========
             K : float
                 (positive) strike price of the option
         
             Returns
             =======
             C0 : float
                 estimated present value of European call option
             '''
             sn = gen_sn(1, I)
             # simulate index level at maturity
             ST = S0 * np.exp((r - 0.5 * sigma ** 2) * T
                          + sigma * np.sqrt(T) * sn[1])
             # calculate payoff at maturity
             hT = np.maximum(ST - K, 0)
             # calculate MCS estimator
             C0 = np.exp(-r * T) * 1 / I * np.sum(hT)
             return C0
----

// code cell end

As a reference, consider the case with a strike price of __K__ = 105:

// code cell start uuid: f325da52-3e45-4e9e-a4a2-067efb1c3bb7
[source, python]
----
In [57]: gbm_mcs_stat(K=105.)
----

----
Out[57]: 10.044221852841922
----

// code cell end

Next, we consider the dynamic simulation approach and allow for European put options in addition to the call option. The function +gbm_mcs_dyna+ implements the algorithm:

// code cell start uuid: 511974d5-5ceb-4b68-bf7f-e01eaa43f7c6
[source, python]
----
In [58]: M = 50
         def gbm_mcs_dyna(K, option='call'):
             ''' Valuation of European options in Black-Scholes-Merton
             by Monte Carlo simulation (of index level paths)
         
             Parameters
             ==========
             K : float
                 (positive) strike price of the option
             option : string
                 type of the option to be valued ('call', 'put')
         
             Returns
             =======
             C0 : float
                 estimated present value of European call option
             '''
             dt = T / M
             # simulation of index level paths
             S = np.zeros((M + 1, I))
             S[0] = S0
             sn = gen_sn(M, I)
             for t in range(1, M + 1):
                 S[t] = S[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt
                         + sigma * np.sqrt(dt) * sn[t])
             # case-based calculation of payoff
             if option == 'call':
                 hT = np.maximum(S[-1] - K, 0)
             else:
                 hT = np.maximum(K - S[-1], 0)
             # calculation of MCS estimator
             C0 = np.exp(-r * T) * 1 / I * np.sum(hT)
             return C0
----

// code cell end

Now, we can compare option price estimates for a call and a put stroke at the same level:

// code cell start uuid: 44ae2961-ec7c-4e69-b6ff-17b8093a894b
[source, python]
----
In [59]: gbm_mcs_dyna(K=110., option='call')
----

----
Out[59]: 7.9500085250284336
----

// code cell end

// code cell start uuid: bedb79ae-4f01-41ea-b16a-22ea9781fc0e
[source, python]
----
In [60]: gbm_mcs_dyna(K=110., option='put')
----

----
Out[60]: 12.629934942682004
----

// code cell end

The question is how well these simulation-based valuation approaches perform relative to the benchmark value from the Black-Scholes-Merton valuation formula. To find out, let us generate respective option values/estimates for a range of strike prices, using the analytical option pricing formula for European calls in Black-Scholes-Merton found in the module ++BSM_Functions.py++:

// code cell start uuid: e9e52ba0-6ccb-46df-a089-49505d6c7919
[source, python]
----
In [61]: from bsm_functions import bsm_call_value
         stat_res = []
         dyna_res = []
         anal_res = []
         k_list = np.arange(80., 120.1, 5.)
         np.random.seed(200000)
         for K in k_list:
             stat_res.append(gbm_mcs_stat(K))
             dyna_res.append(gbm_mcs_dyna(K))
             anal_res.append(bsm_call_value(S0, K, T, r, sigma))
         stat_res = np.array(stat_res)
         dyna_res = np.array(dyna_res)
         anal_res = np.array(anal_res)
----

// code cell end

First, we compare the results from the static simulation approach with precise analytical values:

// code cell start uuid: 3f9f44ec-47de-4891-bf82-2b620c647c9a
[source, python]
----
In [62]: fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 6))
         ax1.plot(k_list, anal_res, 'b', label='analytical')
         ax1.plot(k_list, stat_res, 'ro', label='static')
         ax1.set_ylabel('European call option value')
         ax1.grid(True)
         ax1.legend(loc=0)
         ax1.set_ylim(ymin=0)
         wi = 1.0
         ax2.bar(k_list - wi / 2, (anal_res - stat_res) / anal_res * 100, wi)
         ax2.set_xlabel('strike')
         ax2.set_ylabel('difference in %')
         ax2.set_xlim(left=75, right=125)
         ax2.grid(True)
----

<<opt_val_comp_1>> shows the results. All valuation differences are smaller than 1% absolutely. There are both negative and positive value differences.

[[opt_val_comp_1]]
.Comparison of static and dynamic Monte Carlo estimator values
image::images/pyfi_1015.png[]

// code cell end

A similar picture emerges for the dynamic simulation and valuation approach, whose results are reported in <<opt_val_comp_2>>. Again, all valuation differences are smaller than 1%, absolutely with both positive and negative deviations. As a general rule, the quality of the Monte Carlo estimator can be controlled for by adjusting the number of time intervals __M__ used and/or the number of paths __I__ simulated:

// code cell start uuid: 3f9f44ec-47de-4891-bf82-2b620c647c9a
[source, python]
----
In [63]: fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 6))
         ax1.plot(k_list, anal_res, 'b', label='analytical')
         ax1.plot(k_list, dyna_res, 'ro', label='dynamic')
         ax1.set_ylabel('European call option value')
         ax1.grid(True)
         ax1.legend(loc=0)
         ax1.set_ylim(ymin=0)
         wi = 1.0
         ax2.bar(k_list - wi / 2, (anal_res - dyna_res) / anal_res * 100, wi)
         ax2.set_xlabel('strike')
         ax2.set_ylabel('difference in %')
         ax2.set_xlim(left=75, right=125)
         ax2.grid(True)
----

[[opt_val_comp_2]]
.Comparison of static and dynamic Monte Carlo estimator values
image::images/pyfi_1016.png[]

// code cell end


==== American Options

(((American options, valuation of contingent claims)))(((valuation of contingent claims, American options)))((("contingent claims, valuation of", "American options")))(((optimal stopping problems)))(((Bermudan exercises)))The valuation of American options is more involved compared to European options. In this case, an _optimal stopping_ problem has to be solved to come up with a fair value of the option. <<opt_stop_ch10>> formulates the valuation of an American option as such a problem. The problem formulation is already based on a discrete time grid for use with numerical simulation. In a sense, it is therefore more correct to speak of an option value given _Bermudan_ exercise. For the time interval converging to zero length, the value of the Bermudan option converges to the one of the American option.

[[opt_stop_ch10]]
[latexmath]
.American option prices as optimal stopping problem
++++
\begin{equation*}
V_{0}=\sup_{\tau\in\{0,\Delta t,2\Delta t,...,T\}} e^{-rT}\textbf{E}_{0}^{Q}(h_{\tau}(S_{\tau}))
\end{equation*}
++++

(((Least-Squares Monte Carlo (LSM) algorithm)))(((Monte Carlo simulation, Least-Squares Monte Carlo (LSM) algorithm)))(((continuation value)))The algorithm we describe in the following is called _Least-Squares Monte Carlo_ (LSM) and is from the paper by Longstaff and Schwartz (2001). It can be shown that the value of an American (Bermudan) option at any given date __t__ is given as __V~t~__(__s__) = max(__h~t~__(__s__),__C~t~__(__s__)), where latexmath:[$C_{t}(s)=\textbf{E}_{t}^{Q}(e^{-r \Delta t} V_{t+\Delta t}(S_{t+\Delta t})|S_{t}=s)$] is the so-called _continuation value_ of the option given an index level of __S~t~__ = __s__.

Consider now that we have simulated __I__ paths of the index level over __M__ time intervals of equal size latexmath:[$\Delta t$]. Define __Y~t,i~__ &#x2261; __e__^__–r__&#x1d6e5;__t__^__V__~__t__+&#x1d6e5;__t,i__~ to be the simulated continuation value for path __i__ at time __t__. We cannot use this number directly because it would imply perfect foresight. However, we can use the cross section of all such simulated continuation values to estimate the (expected) continuation value by least-squares regression.

Given a set of basis functions __b~d~__, __d__ = 1,...,__D__, the continuation value is then given by the regression estimate latexmath:[$\hat{C}_{t,i}=\sum_{d=1}^{D}\alpha_{d,t}^{*}\cdot b_{d}(S_{t,i})$], where the optimal regression parameters &#x1d6fc;^*^ are the solution of the least-squares problem stated in <<lsm_reg>>.

[[lsm_reg]]
[latexmath]
.Least-squares regression for American option valuation
++++
\begin{equation*}
\min_{\alpha_{1,t},...,\alpha_{D,t}} \frac{1}{I} \sum_{i=1}^{I}\left(Y_{t,i}-\sum_{d=1}^{D}\alpha_{d,t}\cdot b_{d}(S_{t,i})\right)^{2}\end{equation*}
++++

The function +gbm_mcs_amer+ implements the LSM algorithm for both American call and put options:footnote:[For algorithmic details, refer to Hilpisch (2015).]

// code cell start uuid: 033296d5-230b-4b35-ae3f-a2a7ed8c8937
[source, python]
----
In [64]: def gbm_mcs_amer(K, option='call'):
             ''' Valuation of American option in Black-Scholes-Merton
             by Monte Carlo simulation by LSM algorithm
         
             Parameters
             ==========
             K : float
                 (positive) strike price of the option
             option : string
                 type of the option to be valued ('call', 'put')
         
             Returns
             =======
             C0 : float
                 estimated present value of European call option
             '''
             dt = T / M
             df = np.exp(-r * dt)
             # simulation of index levels
             S = np.zeros((M + 1, I))
             S[0] = S0
             sn = gen_sn(M, I)
             for t in range(1, M + 1):
                 S[t] = S[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt
                         + sigma * np.sqrt(dt) * sn[t])
             # case-based calculation of payoff
             if option == 'call':
                 h = np.maximum(S - K, 0)
             else:
                 h = np.maximum(K - S, 0)
             # LSM algorithm
             V = np.copy(h)
             for t in range(M - 1, 0, -1):
                 reg = np.polyfit(S[t], V[t + 1] * df, 7)
                 C = np.polyval(reg, S[t])
                 V[t] = np.where(C > h[t], V[t + 1] * df, h[t])
             # MCS estimator
             C0 = df * 1 / I * np.sum(V[1])
             return C0
----

// code cell end

// code cell start uuid: 18dba6e2-2a7f-4474-bbee-227f354fcbc3
[source, python]
----
In [65]: gbm_mcs_amer(110., option='call')
----

----
Out[65]: 7.7789332794493156
----

// code cell end

// code cell start uuid: a82c68fc-9820-43a7-8302-3ae0f5a47650
[source, python]
----
In [66]: gbm_mcs_amer(110., option='put')
----

----
Out[66]: 13.614023206242445
----

// code cell end

(((early exercise premium)))The European value of an option represents a lower bound to the American option's value. The difference is generally called the _early exercise premium_. In what follows, we compare European and American option values for the same range of strikes as before to estimate the option premium. This time we take puts:footnote:[Since we do not assume any dividend payments (having an index in mind), there generally is no early exercise premium for call options (i.e., no incentive to exercise the option early).]

// code cell start uuid: 2c4a0f35-5a41-416b-aa39-53d78d1cc366
[source, python]
----
In [67]: euro_res = []
         amer_res = []
         k_list = np.arange(80., 120.1, 5.)
         for K in k_list:
             euro_res.append(gbm_mcs_dyna(K, 'put'))
             amer_res.append(gbm_mcs_amer(K, 'put'))
         euro_res = np.array(euro_res)
         amer_res = np.array(amer_res)
----

// code cell end

(((range="endofrange", startref="ix_valu")))(((range="endofrange", startref="ix_MCvalu")))(((range="endofrange", startref="ix_cont")))<<opt_euro_amer>> shows that for the range of strikes chosen the premium can rise to up pass:[<phrase role="keep-together">to 10%</phrase>]:

// code cell start uuid: 6304932d-114f-43b1-ae59-4b0ad2de33fc
[source, python]
----
In [68]: fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 6))
         ax1.plot(k_list, euro_res, 'b', label='European put')
         ax1.plot(k_list, amer_res, 'ro', label='American put')
         ax1.set_ylabel('call option value')
         ax1.grid(True)
         ax1.legend(loc=0)
         wi = 1.0
         ax2.bar(k_list - wi / 2, (amer_res - euro_res) / euro_res * 100, wi)
         ax2.set_xlabel('strike')
         ax2.set_ylabel('early exercise premium in %')
         ax2.set_xlim(left=75, right=125)
         ax2.grid(True)
----

[[opt_euro_amer]]
.Comparison of European and LSM Monte Carlo estimator values
image::images/pyfi_1017.png[]

// code cell end


=== Risk Measures

((("risk measures", id="ix_risk", range="startofrange")))In addition to valuation, _risk management_ is another important application area of stochastic methods and simulation. This section illustrates the calculation/estimation of two of the most common risk measures applied today in the finance industry.


==== Value-at-Risk

((("risk measures", "value-at-risk (VaR)")))((("value-at-risk (VaR)")))(((tail risk)))_Value-at-risk_ (VaR) is one of the most widely used risk measures, and a much debated one. Loved by practitioners for its intuitive appeal, it is widely discussed and criticized by many--mainly on theoretical grounds, with regard to its limited ability to capture what is called _tail risk_ (more on this shortly). In words, VaR is a number denoted in currency units (e.g., USD, EUR, JPY) indicating a loss (of a portfolio, a single position, etc.) that is not exceeded with some confidence level (probability) over a given period of time.

Consider a stock position, worth 1 million USD today, that has a VaR of 50,000 USD at a confidence level of 99% over a time period of 30 days (one month). This VaR figure says that with a probability of 99% (i.e., in 99 out of 100 cases), the loss to be expected over a period of 30 days will _not exceed_ 50,000 USD. However, it does not say anything about the size of the loss once a loss beyond 50,000 USD occurs—i.e., if the maximum loss is 100,000 or 500,000 USD what the probability of such a specific "higher than VaR loss" is. All it says is that there is a 1% probability that a loss of a _minimum of 50,000 USD or higher_ will occur.

Assume again that we are in a Black-Scholes-Merton setup and consider the following parameterization and simulation of index levels at a future date __T__ = 30/365 (i.e., we assume a period of 30 days):

// code cell start uuid: 5473289e-2301-40fb-a665-2d33d43ea09a
[source, python]
----
In [69]: S0 = 100
         r = 0.05
         sigma = 0.25
         T = 30 / 365.
         I = 10000
         ST = S0 * np.exp((r - 0.5 * sigma ** 2) * T
                      + sigma * np.sqrt(T) * npr.standard_normal(I))
----

// code cell end

To estimate VaR figures, we need the simulated absolute profits and losses relative to the value of the position today in a sorted manner, i.e., from the severest loss to the largest profit:

// code cell start uuid: b2eed114-77e7-479b-b20b-d36a0ffbe636
[source, python]
----
In [70]: R_gbm = np.sort(ST - S0)
----

// code cell end

<<var_hist_gbm>> shows the histogram of the simulated absolute performance values:

// code cell start uuid: b53e5254-96cc-4294-8ef7-76a2cf21cbca
[source, python]
----
In [71]: plt.hist(R_gbm, bins=50)
         plt.xlabel('absolute return')
         plt.ylabel('frequency')
         plt.grid(True)
----

[[var_hist_gbm]]
.Absolute returns of geometric Brownian motion (30d)
image::images/pyfi_1018.png[]

// code cell end

Having the +ndarray+ object with the sorted results, the function +scoreatpercentile+ already does the trick. All we have to do is to define the percentiles (in percent values) in which we are interested. In the +list+ object +percs+, 0.1 translates into a confidence level of 100% – 0.1% = 99.9%. The 30-day VaR given a confidence level of 99.9% in this case is 20.2 currency units, while it is 8.9 at the 90% confidence level:

// code cell start uuid: 768aa308-d5c2-4f5d-9936-c19c9321996a
[source, python]
----
In [72]: percs = [0.01, 0.1, 1., 2.5, 5.0, 10.0]
         var = scs.scoreatpercentile(R_gbm, percs)
         print "%16s %16s" % ('Confidence Level', 'Value-at-Risk')
         print 33 * "-"
         for pair in zip(percs, var):
             print "%16.2f %16.3f" % (100 - pair[0], -pair[1])
----

----
Out[72]: Confidence Level    Value-at-Risk
         ---------------------------------
                    99.99           26.072
                    99.90           20.175
                    99.00           15.753
                    97.50           13.265
                    95.00           11.298
                    90.00            8.942
         
----

// code cell end

As a second example, recall the jump diffusion setup from Merton, which we want to simulate dynamically:

// code cell start uuid: b9952498-c4ad-4d5a-8d3c-3bce1d71006d
[source, python]
----
In [73]: dt = 30. / 365 / M
         rj = lamb * (np.exp(mu + 0.5 * delta ** 2) - 1)
         S = np.zeros((M + 1, I))
         S[0] = S0
         sn1 = npr.standard_normal((M + 1, I))
         sn2 = npr.standard_normal((M + 1, I))
         poi = npr.poisson(lamb * dt, (M + 1, I))
         for t in range(1, M + 1, 1):
             S[t] = S[t - 1] * (np.exp((r - rj - 0.5 * sigma ** 2) * dt
                                + sigma * np.sqrt(dt) * sn1[t])
                                + (np.exp(mu + delta * sn2[t]) - 1)
                                * poi[t])
             S[t] = np.maximum(S[t], 0)
----

// code cell end

// code cell start uuid: 37cfd26e-2c44-456a-8b8b-56cf10e12aac
[source, python]
----
In [74]: R_jd = np.sort(S[-1] - S0)
----

// code cell end

(((fat tails)))In this case, with the jump component having a negative mean, we see something like a bimodal distribution for the simulated profits/losses in <<var_hist_jd>>. From a normal distribution point of view, we have a strongly pronounced left __fat tail__:

// code cell start uuid: 3300cad0-872b-45ef-9b12-3fc3507b2c54
[source, python]
----
In [75]: plt.hist(R_jd, bins=50)
         plt.xlabel('absolute return')
         plt.ylabel('frequency')
         plt.grid(True)
----

[[var_hist_jd]]
.Absolute returns of jump diffusion (30d)
image::images/pyfi_1019.png[]

// code cell end

For this process and parameterization, the VaR over 30 days at the 90% level is almost identical, while it is more than _three times_ as high at the 99.9% level as with the geometric Brownian motion (71.8 vs. 20.2 currency units):

// code cell start uuid: 8adcca19-77bf-4d8e-a342-1a5cc1cadd69
[source, python]
----
In [76]: percs = [0.01, 0.1, 1., 2.5, 5.0, 10.0]
         var = scs.scoreatpercentile(R_jd, percs)
         print "%16s %16s" % ('Confidence Level', 'Value-at-Risk')
         print 33 * "-"
         for pair in zip(percs, var):
             print "%16.2f %16.3f" % (100 - pair[0], -pair[1])
----

----
Out[76]: Confidence Level    Value-at-Risk
         ---------------------------------
                    99.99           75.029
                    99.90           71.833
                    99.00           55.901
                    97.50           45.697
                    95.00           25.993
                    90.00            8.773
         
----

// code cell end

This illustrates the problem of capturing the tail risk so often encountered in financial markets by the standard VaR measure.

To further illustrate the point, we lastly show the VaR measures for both cases in direct comparison graphically. As <<var_comp>> reveals, the VaR measures behave completely differently given a range of typical confidence levels:

// code cell start uuid: 812884b3-c147-4799-8b7a-93eb62a9b1fc
[source, python]
----
In [77]: percs = list(np.arange(0.0, 10.1, 0.1))
         gbm_var = scs.scoreatpercentile(R_gbm, percs)
         jd_var = scs.scoreatpercentile(R_jd, percs)
----

// code cell end

// code cell start uuid: b960f3cc-fed3-4cfa-9189-040931e4ab09
[source, python]
----
In [78]: plt.plot(percs, gbm_var, 'b', lw=1.5, label='GBM')
         plt.plot(percs, jd_var, 'r', lw=1.5, label='JD')
         plt.legend(loc=4)
         plt.xlabel('100 - confidence level [%]')
         plt.ylabel('value-at-risk')
         plt.grid(True)
         plt.ylim(ymax=0.0)
----

[[var_comp]]
.Value-at-risk for geometric Brownian motion and jump diffusion
image::images/pyfi_1020.png[]

// code cell end


==== Credit Value Adjustments

((("risk measures", "credit value adjustments")))((("credit value-at-risk (CVaR)")))(((probability of default)))((("default, probability of")))(((loss level)))(((average loss level)))((("credit value adjustment (CVA)")))Other important risk measures are the credit value-at-risk (CVaR) and the credit value adjustment (CVA), which is derived from the CVaR. Roughly speaking, CVaR is a measure for the risk resulting from the possibility that a counterparty might not be able to honor its obligations—for example, if the counterparty goes bankrupt. In such a case there are two main assumptions to be made: _probability of default_ and the (average) _loss level_.

To make it specific, consider again the benchmark setup of Black-Scholes-Merton with the following parameterization:

// code cell start uuid: 92795f2e-84b4-4881-960f-91a39eb1cc77
[source, python]
----
In [79]: S0 = 100.
         r = 0.05
         sigma = 0.2
         T = 1.
         I = 100000
         ST = S0 * np.exp((r - 0.5 * sigma ** 2) * T
                      + sigma * np.sqrt(T) * npr.standard_normal(I))
----

// code cell end

In the simplest case, one considers a fixed (average) loss level __L__ and a fixed probability __p__ for default (per year) of a counterparty:

// code cell start uuid: 3e3c6a61-c268-44f4-bce9-f3c2f83faac9
[source, python]
----
In [80]: L = 0.5
----

// code cell end

// code cell start uuid: f06f2c7d-8c1a-4cc3-b171-dad76994c6b9
[source, python]
----
In [81]: p = 0.01
----

// code cell end

Using the Poisson distribution, default scenarios are generated as follows, taking into account that a default can only occur once:

// code cell start uuid: 38b71c82-76a1-4299-992f-93820cbf2677
[source, python]
----
In [82]: D = npr.poisson(p * T, I)
         D = np.where(D > 1, 1, D)
----

// code cell end

Without default, the risk-neutral value of the future index level should be equal to the current value of the asset today (up to differences resulting from numerical errors):

// code cell start uuid: 46418aea-2253-4f09-840a-1c45676bda2c
[source, python]
----
In [83]: np.exp(-r * T) * 1 / I * np.sum(ST)
----

----
Out[83]: 99.981825216842921
----

// code cell end

The CVaR under our assumptions is calculated as follows:

// code cell start uuid: fe7436d3-4eb4-40f4-9d4c-c5efa0e3d3a0
[source, python]
----
In [84]: CVaR = np.exp(-r * T) * 1 / I * np.sum(L * D * ST)
         CVaR
----

----
Out[84]: 0.5152011134161355
----

// code cell end

Analogously, the present value of the asset, adjusted for the credit risk, is given as follows:

// code cell start uuid: 3070c8f6-8a77-4373-b423-f6871170dbaf
[source, python]
----
In [85]: S0_CVA = np.exp(-r * T) * 1 / I * np.sum((1 - L * D) * ST)
         S0_CVA
----

----
Out[85]: 99.466624103426781
----

// code cell end

This should be (roughly) the same as subtracting the CVaR value from the current asset value:

// code cell start uuid: d7d14139-b76d-4c11-a57b-930db11abd3c
[source, python]
----
In [86]: S0_adj = S0 - CVaR
         S0_adj
----

----
Out[86]: 99.48479888658386
----

// code cell end

In this particular simulation example, we observe roughly 1,000 losses due to credit risk, which is to be expected given the assumed default probability of 1% and 100,000 simulated paths:

// code cell start uuid: c6995617-5021-4d8f-9f94-8fca0571ff89
[source, python]
----
In [87]: np.count_nonzero(L * D * ST)
----

----
Out[87]: 1031
----

// code cell end

<<cva_hist_stock>> shows the complete frequency distribution of the losses due to a default. Of course, in the large majority of cases (i.e., in about 99,000 of the 100,000 cases) there is no loss to observe:

// code cell start uuid: fc6e6717-9ffc-486c-a736-3892c277f3e6
[source, python]
----
In [88]: plt.hist(L * D * ST, bins=50)
         plt.xlabel('loss')
         plt.ylabel('frequency')
         plt.grid(True)
         plt.ylim(ymax=175)
----

[[cva_hist_stock]]
.Losses due to risk-neutrally expected default (stock)
image::images/pyfi_1021.png[]

// code cell end

Consider now the case of a European call option. Its value is about 10.4 currency units at a strike of 100:

// code cell start uuid: 59b7c831-c915-4c06-a23b-0ac913220d76
[source, python]
----
In [89]: K = 100.
         hT = np.maximum(ST - K, 0)
         C0 = np.exp(-r * T) * 1 / I * np.sum(hT)
         C0
----

----
Out[89]: 10.427336109660052
----

// code cell end

The CVaR is about 5 cents given the same assumptions with regard to probability of default and loss level:

// code cell start uuid: da0198e3-10bc-4324-8e0e-b09c2e61e94d
[source, python]
----
In [90]: CVaR = np.exp(-r * T) * 1 / I * np.sum(L * D * hT)
         CVaR
----

----
Out[90]: 0.053822578452208093
----

// code cell end

Accordingly, the adjusted option value is roughly 5 cents lower:

// code cell start uuid: 24d26328-f3f2-4da4-8d5c-7fb06a70eec8
[source, python]
----
In [91]: C0_CVA = np.exp(-r * T) * 1 / I * np.sum((1 - L * D) * hT)
         C0_CVA
----

----
Out[91]: 10.373513531207843
----

// code cell end

Compared to the case of a regular asset, the option case has somewhat different characteristics. We only see a little more than 500 losses due to a default, although we again have about 1,000 defaults. This results from the fact that the payoff of the option at maturity has a high probability of being zero:

// code cell start uuid: a221dbb8-eec3-45e1-abd7-146050c0285f
[source, python]
----
In [92]: np.count_nonzero(L * D * hT)  # number of losses
----

----
Out[92]: 582
----

// code cell end

// code cell start uuid: e1becbb6-7a1e-49bb-8a8e-b7daab189c6e
[source, python]
----
In [93]: np.count_nonzero(D)  # number of defaults
----

----
Out[93]: 1031
----

// code cell end

// code cell start uuid: 44c3d031-8002-4bba-abd7-0db5451b2d52
[source, python]
----
In [94]: I - np.count_nonzero(hT)  # zero payoff
----

----
Out[94]: 43995
----

// code cell end

(((range="endofrange", startref="ix_risk")))<<cva_hist_opt>> shows that the CVaR for the option has a completely different frequency distribution compared to the regular asset case:

// code cell start uuid: b132d24e-093b-45e6-a4cc-29b8ef006038
[source, python]
----
In [95]: plt.hist(L * D * hT, bins=50)
         plt.xlabel('loss')
         plt.ylabel('frequency')
         plt.grid(True)
         plt.ylim(ymax=350)
----

[[cva_hist_opt]]
.Losses due to risk-neutrally expected default (call option)
image::images/pyfi_1022.png[]

// code cell end


=== Conclusions

This chapter deals with methods and techniques important to the application of Monte Carlo simulation in finance. In particular, it shows how to _generate (pseudo)random numbers_ based on different distribution laws. It proceeds with the _simulation of random variables and stochastic processes_, which is important in many financial areas. Two application areas are discussed in some depth in this chapter: _valuation of options_ with European and American exercise and the _estimation of risk measures_ like value-at-risk and credit value adjustments.

The chapter illustrates that +Python+ in combination with +NumPy+ is well suited to implementing even such computationally demanding tasks as the valuation of American options by Monte Carlo simulation. This is mainly due to the fact that the majority of functions and classes of +NumPy+ are implemented in +C+, which leads to considerable speed advantages in general over pure +Python+ code. A further benefit is the compactness and readability of the resulting code due to vectorized operations.


=== Further Reading

The original article introducing Monte Carlo simulation to finance is:

* Boyle, Phelim (1977): "Options: A Monte Carlo Approach." _Journal of Financial Economics_, Vol. 4, No. 4, pp. 322–338.

Other original papers cited in this chapter are (see also <<model_simulation>>):

* Black, Fischer and Myron Scholes (1973): "The Pricing of Options and Corporate Liabilities." __Journal of Political Economy__, Vol. 81, No. 3, pp. 638–659.

* Cox, John, Jonathan Ingersoll and Stephen Ross (1985): "A Theory of the Term Structure of Interest Rates." __Econometrica__, Vol. 53, No. 2, pp. 385–407.

* Heston, Steven (1993): "A Closed-From Solution for Options with Stochastic Volatility with Applications to Bond and Currency Options." __The Review of Financial Studies__, Vol. 6, No. 2, 327–343.

* Merton, Robert (1973): "Theory of Rational Option Pricing." __Bell Journal of Economics and Management Science__, Vol. 4, pp. 141–183.

* Merton, Robert (1976): "Option Pricing When the Underlying Stock Returns Are Discontinuous." __Journal of Financial Economics__, Vol. 3, No. 3, pp. 125–144.

The books by Glassermann (2004) and Hilpisch (2015) cover all topics of this chapter in depth (however, the first one does not cover any technical implementation details):

* Glasserman, Paul (2004): _Monte Carlo Methods in Financial Engineering_. Springer, New York.
* Hilpisch, Yves (2015): _Derivatives Analytics with Python_. Wiley Finance, Chichester, England. http://www.derivatives-analytics-with-python.com[].

It took until the turn of the century for an efficient method to value American options by Monte Carlo simulation to finally be published:

* Longstaff, Francis and Eduardo Schwartz (2001): "Valuing American Options by Simulation: A Simple Least Squares Approach." _Review of Financial Studies_, Vol. 14, No. 1, pp. 113–147.

A broad and in-depth treatment of credit risk is provided in:

* Duffie, Darrell and Kenneth Singleton (2003): _Credit Risk--Pricing, Measurement, and Management_. Princeton University Press, Princeton, NJ.

