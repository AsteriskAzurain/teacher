[[math_tools]]


== Mathematical Tools

[quote, Bill Gaede]
____
[role="align_me_right"]
The mathematicians are the priests of the modern world.
____

((("finance", "mathematical tools for", id="ix_Fmt", range="startofrange")))Since the arrival of the so-called Rocket Scientists on Wall Street in the '80s and '90s, finance has evolved into a discipline of applied mathematics. While early research papers in finance came with few mathematical expressions and equations, current ones are mainly comprised of mathematical expressions and equations, with some explanatory text around.

This chapter introduces a number of useful mathematical tools for finance, without providing a detailed background for each of them. There are many useful books on this topic available. Therefore, this chapter focuses on how to use the tools and techniques with +Python+. Among other topics, it covers:

Approximation:: Regression and interpolation are among the most often used numerical techniques in finance.
Convex optimization:: A number of financial disciplines need tools for convex optimization (e.g., option pricing when it comes to model calibration).
Integration:: In particular, the valuation of financial (derivative) assets often boils down to the evaluation of integrals.
Symbolic mathematics:: +Python+ provides with +SymPy+ a powerful tool for symbolic mathematics, e.g., to solve (systems of) equations.


=== Approximation

((("approximation of functions", id="ix_app", range="startofrange")))((("mathematical tools", "approximation of functions", id="ix_MTapp", range="startofrange")))(((NumPy, importing)))(((matplotlib library, importing matplotlib.pyplot)))(((libraries, importing)))To begin with, let us import the libraries that we need for the moment—++NumPy++ and +matplotlib.pyplot+:

// code cell start uuid: 460b709e-eed1-48e4-b3ad-d07377ea5de6
[source, python]
----
In [1]: import numpy as np
        import matplotlib.pyplot as plt
        %matplotlib inline
----

// code cell end

Throughout this discussion, the main example function we will use is the following, which is comprised of a trigonometric term and a linear term:

// code cell start uuid: 2326c3ad-f244-4f48-8b68-851bd2347d57
[source, python]
----
In [2]: def f(x):
            return np.sin(x) + 0.5 * x
----

// code cell end

The main focus is the approximation of this function over a given interval by _regression_ and _interpolation_. First, let us generate a plot of the function to get a better view of what exactly the approximation shall achieve. The interval of interest shall be [–2&#x1d70b;,2&#x1d70b;]. <<sin_plot>> displays the function over the fixed interval defined via the +linspace+ function. ++np.linspace(++__++start++__++,++ __++stop++__++,++ __++num++__++)++ returns _++num++_ points beginning with _++start++_ and ending with _++stop++_, with the subintervals between two consecutive points being evenly spaced:

// code cell start uuid: c09f73d2-c2a5-4c6d-a2f1-08a191378417
[source, python]
----
In [3]: x = np.linspace(-2 * np.pi, 2 * np.pi, 50)
----

// code cell end

// code cell start uuid: 96d2bd1b-8883-486d-920d-b610aeb076a8
[source, python]
----
In [4]: plt.plot(x, f(x), 'b')
        plt.grid(True)
        plt.xlabel('x')
        plt.ylabel('f(x)')
----

[[sin_plot]]
.Example function plot
image::images/pyfi_0901.png[]

// code cell end


==== Regression

((("approximation of functions", "regression", id="ix_appreg", range="startofrange")))(((regression analysis, mathematical tools for, strengths of)))(((observation points)))(((independent observations)))(((dependent observations)))Regression is a rather efficient tool when it comes to function approximation. It is not only suited to approximate one-dimensional functions but also works well in higher dimensions. The numerical techniques needed to come up with regression results are easily implemented and quickly executed. Basically, the task of regression, given a set of so-called basis functions __b~d~__, __d__ &#x2208; {1,...,__D__}, is to find optimal parameters latexmath:[$\alpha_{1}^*,...,\alpha_{D}^*$] according to <<reg_problem>>, where __y~i~__ &#x2261; __f__(__x~i~__) for __i__ &#x2208; \{1,&#x22ef;, __I__} observation points. The __x~i~__ are considered _independent_ observations and the __y~i~ dependent__ observations (in a functional or statistical sense).

[[reg_problem]]
[latexmath]
.Minimization problem of regression
++++
\begin{equation*}
\min_{\alpha_{1},...,\alpha_{D}} \frac{1}{I} \sum_{i=1}^{I} \left( y_{i}-\sum_{d=1}^{D}\alpha_{d}\cdot b_{d}(x_{i}) \right) ^{2}
\end{equation*}
++++


===== Monomials as basis functions

(((regression analysis, mathematical tools for, monomials as basis functions)))One of the simplest cases is to take monomials as basis functions—i.e., __b__~1~ = 1, __b__~2~ = __x__, __b__~3~ = __x__^2^, __b__~4~ = __x__^3^,.... In such a case, +NumPy+ has built-in functions for both the determination of the optimal parameters (namely, +polyfit+) and the evaluation of the approximation given a set of input values (namely, +polyval+).

(((polyfit function)))<<polyfit>> lists the parameters the +polyfit+ function takes. Given the returned optimal regression coefficients _++p++_ from +polyfit+, +np.polyval+(_++p++_, _++x++_) then returns the regression values for the +x+ coordinates.

[[polyfit]]
.Parameters of polyfit function
[options="header, unbreakable"]
|=======
|Parameter    |Description
|+x+     | +x+ coordinates (independent variable values)
|+y+     | +y+ coordinates (dependent variable values)
|+deg+     | Degree of the fitting polynomial
|+full+     | If +True+, returns diagnostic information in addition
|+w+     | Weights to apply to the +y+ coordinates
|+cov+     | If +True+, covariance matrix is also returned
|=======

In typical vectorized fashion, the application of +polyfit+ and +polyval+ takes on the following form for a linear regression (i.e., for +deg=1+):

// code cell start uuid: ace90420-7219-4227-8210-bf107f556726
[source, python]
----
In [5]: reg = np.polyfit(x, f(x), deg=1)
        ry = np.polyval(reg, x)
----

// code cell end

Given the regression estimates stored in the +ry+ array, we can compare the regression result with the original function as presented in <<sin_plot_reg_1>>. Of course, a linear regression cannot account for the +sin+ part of the example function:

// code cell start uuid: c0667d3e-a48a-413d-b250-5e0d3b58275e
[source, python]
----
In [6]: plt.plot(x, f(x), 'b', label='f(x)')
        plt.plot(x, ry, 'r.', label='regression')
        plt.legend(loc=0)
        plt.grid(True)
        plt.xlabel('x')
        plt.ylabel('f(x)')
----

[[sin_plot_reg_1]]
.Example function and linear regression
image::images/pyfi_0902.png[]

// code cell end

To account for the +sin+ part of the example function, higher-order monomials are necessary. The next regression attempt takes monomials up to the order of 5 as basis functions. It should not be too surprising that the regression result, as seen in <<sin_plot_reg_2>>, now looks much closer to the original function. However, it is still far away from being perfect:

// code cell start uuid: 096bb07a-55f7-45de-8734-2a76d8749d53
[source, python]
----
In [7]: reg = np.polyfit(x, f(x), deg=5)
        ry = np.polyval(reg, x)
----

// code cell end

// code cell start uuid: 5e17309e-e8e2-4df9-b841-0f57d983b89e
[source, python]
----
In [8]: plt.plot(x, f(x), 'b', label='f(x)')
        plt.plot(x, ry, 'r.', label='regression')
        plt.legend(loc=0)
        plt.grid(True)
        plt.xlabel('x')
        plt.ylabel('f(x)')
----

// code cell end

The last attempt takes monomials up to order 7 to approximate the example function. In this case the result, as presented in <<sin_plot_reg_3>>, is quite convincing:

// code cell start uuid: 67b14a21-e8f2-4dd4-a43b-0d232f2b4055
[source, python]
----
In [9]: reg = np.polyfit(x, f(x), 7)
        ry = np.polyval(reg, x)
----

// code cell end

[[sin_plot_reg_2]]
.Regression with monomials up to order 5
image::images/pyfi_0903.png[]

// code cell start uuid: 053752b7-7eb3-4d93-acdf-69874ceada12
[source, python]
----
In [10]: plt.plot(x, f(x), 'b', label='f(x)')
         plt.plot(x, ry, 'r.', label='regression')
         plt.legend(loc=0)
         plt.grid(True)
         plt.xlabel('x')
         plt.ylabel('f(x)')
----

[[sin_plot_reg_3]]
.Regression with monomials up to order 7
image::images/pyfi_0904.png[]

// code cell end

A brief check reveals that the result is not perfect:

// code cell start uuid: e600b6be-4cf2-4212-807a-7f397f081e98
[source, python]
----
In [11]: np.allclose(f(x), ry)
----

----
Out[11]: False
----

// code cell end

However, the mean squared error (MSE) is not too large--at least, over this narrow range of +x+ values:

// code cell start uuid: bc6918fe-f520-483c-94eb-41dd89abfa70
[source, python]
----
In [12]: np.sum((f(x) - ry) ** 2) / len(x)
----

----
Out[12]: 0.0017769134759517413
----

// code cell end


===== Individual basis functions

(((regression analysis, mathematical tools for, individual basis functions)))In general, you can reach better regression results when you can choose better sets of basis functions, e.g., by exploiting knowledge about the function to approximate. In this case, the individual basis functions have to be defined via a matrix approach (i.e., using a +NumPy+ +ndarray+ object). First, the case with monomials up to order 3:

// code cell start uuid: b4f05890-56e0-4f29-9d61-bd9948ad8af0
[source, python]
----
In [13]: matrix = np.zeros((3 + 1, len(x)))
         matrix[3, :] = x ** 3
         matrix[2, :] = x ** 2
         matrix[1, :] = x
         matrix[0, :] = 1
----

// code cell end

(((NumPy, numpy.linalg sublibrary)))The sublibrary +numpy.linalg+ provides the function +lstsq+ to solve least-squares optimization problems like the one in <<reg_problem>>:

// code cell start uuid: c8963eee-4bc8-4ef2-a172-d4b64fd065a3
[source, python]
----
In [14]: reg = np.linalg.lstsq(matrix.T, f(x))[0]
----

// code cell end

Applying +lstsq+ to our problem in this way yields the optimal parameters for the single basis functions:

// code cell start uuid: efd077d1-9c8a-4961-be95-400f83cd679e
[source, python]
----
In [15]: reg
----

----
Out[15]: array([  1.13968447e-14,   5.62777448e-01,  -8.88178420e-16,
                 -5.43553615e-03])
----

// code cell end

(((dot function)))To get the regression estimates we apply the +dot+ function to the +reg+ and +matrix+ arrays. <<sin_plot_reg_4>> shows the result. +np.dot+(_++a++_, _++b++_) simply gives the dot product for the two arrays _++a++_ and _++b++_:

// code cell start uuid: efb7b252-d0f8-4263-b2be-4d9588ab06a7
[source, python]
----
In [16]: ry = np.dot(reg, matrix)
----

// code cell end

// code cell start uuid: 1b1953fe-83a2-436b-8cd4-69c5abf6d2e1
[source, python]
----
In [17]: plt.plot(x, f(x), 'b', label='f(x)')
         plt.plot(x, ry, 'r.', label='regression')
         plt.legend(loc=0)
         plt.grid(True)
         plt.xlabel('x')
         plt.ylabel('f(x)')
----



// code cell end

(((least-squares function)))The result in <<sin_plot_reg_4>> is not really as good as expected based on our previous experience with monomials. Using the more general approach allows us to exploit our knowledge about the example function. We know that there is a +sin+ part in the function. Therefore, it makes sense to include a sine function in the set of basis functions. For simplicity, we replace the highest-order monomial:

// code cell start uuid: ac77ef01-8abe-4b99-8f92-8325a396ff2c
[source, python]
----
In [18]: matrix[3, :] = np.sin(x)
         reg = np.linalg.lstsq(matrix.T, f(x))[0]
         ry = np.dot(reg, matrix)
----

// code cell end

[[sin_plot_reg_4]]
.Regression via least-squares function
image::images/pyfi_0905.png[]

<<sin_plot_reg_5>> illustrates that the regression is now pretty close to the original function:

// code cell start uuid: 58d9db31-5885-4fba-8ae7-2e962a0963ca
[source, python]
----
In [19]: plt.plot(x, f(x), 'b', label='f(x)')
         plt.plot(x, ry, 'r.', label='regression')
         plt.legend(loc=0)
         plt.grid(True)
         plt.xlabel('x')
         plt.ylabel('f(x)')
----

[[sin_plot_reg_5]]
.Regression using individual functions
image::images/pyfi_0906.png[]

// code cell end

Indeed, the regression now is "perfect" in a numerical sense:

// code cell start uuid: 02481bd5-c737-46bc-9b90-5554fcad8745
[source, python]
----
In [20]: np.allclose(f(x), ry)
----

----
Out[20]: True
----

// code cell end

// code cell start uuid: 6bf80137-3a52-483b-a557-b092bbf23b36
[source, python]
----
In [21]: np.sum((f(x) - ry) ** 2) / len(x)
----

----
Out[21]: 2.2749084503102031e-31
----

// code cell end

In fact, the minimization routine recovers the correct parameters of 1 for the +sin+ part and 0.5 for the linear part:

// code cell start uuid: 86f9a92c-600d-4515-b34d-20c9f35a86b0
[source, python]
----
In [22]: reg
----

----
Out[22]: array([  1.55428020e-16,   5.00000000e-01,   0.00000000e+00,
                  1.00000000e+00])
----

// code cell end


===== Noisy data

(((regression analysis, mathematical tools for, noisy data and)))(((noisy data)))(((data, noisy data)))(((simulation, noisy data from)))Regression can cope equally well with _noisy_ data, be it data from simulation or from (non-perfect) measurements. To illustrate this point, let us generate both independent observations with noise and also dependent observations with noise:

// code cell start uuid: 75d3a6a6-a940-4a49-b35d-29f21880ab95
[source, python]
----
In [23]: xn = np.linspace(-2 * np.pi, 2 * np.pi, 50)
         xn = xn + 0.15 * np.random.standard_normal(len(xn))
         yn = f(xn) + 0.25 * np.random.standard_normal(len(xn))
----

// code cell end

The very regression is the same:

// code cell start uuid: f6f9c05f-1f96-48ee-aaca-f4d80c3d3ac5
[source, python]
----
In [24]: reg = np.polyfit(xn, yn, 7)
         ry = np.polyval(reg, xn)
----

// code cell end

<<sin_plot_reg_6>> reveals that the regression results are closer to the original function than the noisy data points. In a sense, the regression averages out the noise to some extent:

// code cell start uuid: 9a475222-3bfd-4300-951b-94e60792c6da
[source, python]
----
In [25]: plt.plot(xn, yn, 'b^', label='f(x)')
         plt.plot(xn, ry, 'ro', label='regression')
         plt.legend(loc=0)
         plt.grid(True)
         plt.xlabel('x')
         plt.ylabel('f(x)')
----

[[sin_plot_reg_6]]
.Regression with noisy data
image::images/pyfi_0907.png[]

// code cell end


===== Unsorted data

(((regression analysis, mathematical tools for, unsorted data and)))(((unsorted data)))(((data, unsorted data)))Another important aspect of regression is that the approach also works seamlessly with unsorted data. The previous examples all rely on sorted +x+ data. This does not have to be the case. To make the point, let us randomize the independent data points as follows:

// code cell start uuid: 8ea85cdb-47f2-4967-b684-7894d9964e76
[source, python]
----
In [26]: xu = np.random.rand(50) * 4 * np.pi - 2 * np.pi
         yu = f(xu)
----

// code cell end

In this case, you can hardly identify any structure by just visually inspecting the pass:[<phrase role='keep-together'>raw data:</phrase>]

// code cell start uuid: 0034edf5-1cef-4eea-be44-c69103fe6eb2
[source, python]
----
In [27]: print xu[:10].round(2)
         print yu[:10].round(2)
----

----
Out[27]: [ 4.09  0.5   1.48 -1.85  1.65  4.51 -5.7   1.83  4.42 -4.2 ]
         [ 1.23  0.72  1.74 -1.89  1.82  1.28 -2.3   1.88  1.25 -1.23]
         
----

// code cell end

(((observation points)))As with the noisy data, the regression approach does not care for the order of the observation points. This becomes obvious upon inspecting the structure of the minimization problem in <<reg_problem>>. It is also obvious by the results, as presented in <<sin_plot_reg_7>>:

// code cell start uuid: d7f5f003-1cb8-4432-a8d6-cb4bef1a101a
[source, python]
----
In [28]: reg = np.polyfit(xu, yu, 5)
         ry = np.polyval(reg, xu)
----

// code cell end

// code cell start uuid: 40177962-0363-479c-bdbd-451a4c043060
[source, python]
----
In [29]: plt.plot(xu, yu, 'b^', label='f(x)')
         plt.plot(xu, ry, 'ro', label='regression')
         plt.legend(loc=0)
         plt.grid(True)
         plt.xlabel('x')
         plt.ylabel('f(x)')
----

[[sin_plot_reg_7]]
.Regression with unsorted data
image::images/pyfi_0908.png[]

// code cell end


===== Multiple dimensions

(((regression analysis, mathematical tools for, multiple dimensions and)))(((multiple dimensions)))Another convenient characteristic of the least-squares regression approach is that it carries over to multiple dimensions without too many modifications. As an example function we take +fm+, as presented next:

// code cell start uuid: 82b95a7b-9e3e-4dc8-b313-1af775b06b8b
[source, python]
----
In [30]: def fm((x, y)):
             return np.sin(x) + 0.25 * x + np.sqrt(y) + 0.05 * y ** 2
----

// code cell end

To visualize this function, we need a grid of (independent) data points:

// code cell start uuid: b03b67ac-f7df-41d1-9dab-1e074e8738fa
[source, python]
----
In [31]: x = np.linspace(0, 10, 20)
         y = np.linspace(0, 10, 20)
         X, Y = np.meshgrid(x, y)
           # generates 2-d grids out of the 1-d arrays
         Z = fm((X, Y))
         x = X.flatten()
         y = Y.flatten()
           # yields 1-d arrays from the 2-d grids
----

// code cell end

Based on the grid of independent and dependent data points as embodied now by +X, Y, Z+, <<sin_plot_3d_1>> presents the shape of the function +fm+:

// code cell start uuid: 52a91ef7-33c4-4de1-b69b-ea4d740aa252
[source, python]
----
In [32]: from mpl_toolkits.mplot3d import Axes3D
         import matplotlib as mpl
         
         fig = plt.figure(figsize=(9, 6))
         ax = fig.gca(projection='3d')
         surf = ax.plot_surface(X, Y, Z, rstride=2, cstride=2,
                 cmap=mpl.cm.coolwarm,
                 linewidth=0.5, antialiased=True)
         ax.set_xlabel('x')
         ax.set_ylabel('y')
         ax.set_zlabel('f(x, y)')
         fig.colorbar(surf, shrink=0.5, aspect=5)
----

[[sin_plot_3d_1]]
.Function with two parameters
image::images/pyfi_0909.png[]

// code cell end

To get good regression results we compile a set of basis functions, including both a +sin+ and a +sqrt+ function, which leverages our knowledge of the example function:

// code cell start uuid: 5918f2cf-3ead-4b80-980e-4a375ee159db
[source, python]
----
In [33]: matrix = np.zeros((len(x), 6 + 1))
         matrix[:, 6] = np.sqrt(y)
         matrix[:, 5] = np.sin(x)
         matrix[:, 4] = y ** 2
         matrix[:, 3] = x ** 2
         matrix[:, 2] = y
         matrix[:, 1] = x
         matrix[:, 0] = 1
----

// code cell end

(((statmodels library)))((("ordinary least-squares regression (OLS)")))The +statsmodels+ library offers the quite general and helpful function +OLS+ for least-squares regression both in one dimension and multiple dimensions:footnote:[For details on the use of +OLS+, refer to the http://bit.ly/using_ols[documentation].]

// code cell start uuid: 48ce242e-3411-41bf-b6c1-c8e228f3e494
[source, python]
----
In [34]: import statsmodels.api as sm
----

// code cell end

// code cell start uuid: b9eb74bd-9280-4d8b-ae7d-8853911389cb
[source, python]
----
In [35]: model = sm.OLS(fm((x, y)), matrix).fit()
----

// code cell end

(((coefficient of determination)))One advantage of using the +OLS+ function is that it provides a wealth of additional information about the regression and its quality. A summary of the results is accessed by calling +model.summary+. Single statistics, like the _coefficient of determination_, can in general also be accessed directly:

// code cell start uuid: cc9c9f77-9051-4d82-bb7b-c646216d542f
[source, python]
----
In [36]: model.rsquared
----

----
Out[36]: 1.0
----

// code cell end

For our purposes, we of course need the optimal regression parameters, which are stored in the +params+ attribute of our +model+ object:

// code cell start uuid: b68898de-9001-4a15-a5ee-7ee50c5592ea
[source, python]
----
In [37]: a = model.params
         a
----

----
Out[37]: array([  7.14706072e-15,   2.50000000e-01,  -2.22044605e-16,
                 -1.02348685e-16,   5.00000000e-02,   1.00000000e+00,
                  1.00000000e+00])
----

// code cell end

(((reg_func function)))The function +reg_func+ gives back, for the given optimal regression parameters and the indpendent data points, the function values for the regression function:

// code cell start uuid: 1f5d1574-8238-45e4-987d-bdb7cc6b5e03
[source, python]
----
In [38]: def reg_func(a, (x, y)):
             f6 = a[6] * np.sqrt(y)
             f5 = a[5] * np.sin(x)
             f4 = a[4] * y ** 2
             f3 = a[3] * x ** 2
             f2 = a[2] * y
             f1 = a[1] * x
             f0 = a[0] * 1
             return (f6 + f5 + f4 + f3 +
                     f2 + f1 + f0)
----

// code cell end

These values can then be compared with the original shape of the example function, as shown in <<sin_plot_3d_2>>:

// code cell start uuid: fd9f8eed-1b10-4ec8-8a15-bedae7f53a18
[source, python]
----
In [39]: RZ = reg_func(a, (X, Y))
----

// code cell end

// code cell start uuid: 096451ce-173a-43b5-b81f-9dac26df2702
[source, python]
----
In [40]: fig = plt.figure(figsize=(9, 6))
         ax = fig.gca(projection='3d')
         surf1 = ax.plot_surface(X, Y, Z, rstride=2, cstride=2,
                     cmap=mpl.cm.coolwarm, linewidth=0.5,
                     antialiased=True)
         surf2 = ax.plot_wireframe(X, Y, RZ, rstride=2, cstride=2,
                                   label='regression')
         ax.set_xlabel('x')
         ax.set_ylabel('y')
         ax.set_zlabel('f(x, y)')
         ax.legend()
         fig.colorbar(surf, shrink=0.5, aspect=5)
----

[[sin_plot_3d_2]]
.Higher-dimension regression
image::images/pyfi_0910.png[]

// code cell end

.Regression
[TIP]
====
(((range="endofrange", startref="ix_appreg")))Least-squares regression approaches have multiple areas of application, including simple function approximation and function approximation based on noisy or unsorted data. These approaches can be applied to single as well as multidimensional problems. Due to the underlying mathematics, the application is always "almost the same."
====


==== Interpolation

(((approximation of functions, interpolation)))((("interpolation", id="ix_int", range="startofrange")))(((cubic splines)))(((spline interpolation)))Compared to regression, _interpolation_ (e.g., with cubic splines), is much more involved mathematically. It is also limited to low-dimensional problems. Given an ordered set of observation points (ordered in the +x+ dimension), the basic idea is to do a regression between two neighboring data points in such a way that not only are the data points perfectly matched by the resulting, piecewise-defined interpolation function, but also that the function is continuously differentiable at the data points. Continuous differentiability requires at least interpolation of degree 3--i.e., with _cubic_ splines. However, the approach also works in general with quadratic and even linear splines. First, the importing of the respective sublibrary:

// code cell start uuid: 5e345542-f425-4630-9eae-0a446927ad73
[source, python]
----
In [41]: import scipy.interpolate as spi
----

// code cell end

// code cell start uuid: 20200433-3e90-4bb6-be5a-8e6a1360989e
[source, python]
----
In [42]: x = np.linspace(-2 * np.pi, 2 * np.pi, 25)
----

// code cell end

We take again the original example function for illustration purposes:

// code cell start uuid: 6e3caa90-2a2a-4429-8536-ab25091a5640
[source, python]
----
In [43]: def f(x):
             return np.sin(x) + 0.5 * x
----

// code cell end

(((splrep function)))(((splev function)))The application itself, given an ++x++-ordered set of data points, is as simple as the application of +polyfit+ and +polyval+. Here, the respective functions are +splrep+ and +splev+. <<splrep>> lists the major parameters that the +splrep+ function takes.

[[splrep]]
.Parameters of splrep function
[options="header, unbreakable"]
|=======
|Parameter    |Description
|+x+     | (Ordered) +x+ coordinates (independent variable values)
|+y+     | (++x++-ordered) +y+ coordinates (dependent variable values)
|+w+     | Weights to apply to the +y+ coordinates
|+xb+, +xe+     | Interval to fit, if +None+ +[x[0], x[-1]]+
|+k+     | Order of the spline fit (+1 \<= k \<= 5+)
|+s+     | Smoothing factor (the larger, the more smoothing)
|+full_output+     | If +True+ additional output is returned
|+quiet+     | If +True+ suppress messages
|=======

<<splev>> lists the parameters that the splev function takes.

[[splev]]
.Parameters of splev function
[options="header, unbreakable"]
|=======
|Parameter    |Description
|+x+       | (Ordered) +x+ coordinates (independent variable values)
|+tck+     | Sequence of length 3 returned by +splrep+ (knots, coefficients, degree)
|+der+     | Order of derivative (0 for function, 1 for first derivative)
|+ext+     | Behavior if +x+ not in knot sequence (0 extrapolate, 1 return 0, 2 raise +ValueError+)
|=======

Applied to the current example, this translates into the following:

// code cell start uuid: afdb9c45-1b12-4358-ba3e-a1bc3c4d32db
[source, python]
----
In [44]: ipo = spi.splrep(x, f(x), k=1)
----

// code cell end

// code cell start uuid: 7217594a-3398-4dfd-9d16-f90ee0f24bd9
[source, python]
----
In [45]: iy = spi.splev(x, ipo)
----

// code cell end

As <<sin_plot_ipo_1>> shows, the interpolation already seems really good with linear splines (i.e., +k=1+):

// code cell start uuid: 89d96d0e-dabd-490f-acad-15bba904b546
[source, python]
----
In [46]: plt.plot(x, f(x), 'b', label='f(x)')
         plt.plot(x, iy, 'r.', label='interpolation')
         plt.legend(loc=0)
         plt.grid(True)
         plt.xlabel('x')
         plt.ylabel('f(x)')
----

[[sin_plot_ipo_1]]
.Example plot with linear interpolation
image::images/pyfi_0911.png[]

// code cell end

This can be confirmed numerically:

// code cell start uuid: b9cacc73-786b-4c6b-ad5f-e4d22729c17e
[source, python]
----
In [47]: np.allclose(f(x), iy)
----

----
Out[47]: True
----

// code cell end

Spline interpolation is often used in finance to generate estimates for dependent values of independent data points not included in the original observations. To this end, let us pick a much smaller interval and have a closer look at the interpolated values with the linear splines:

// code cell start uuid: 3daf1f42-54d8-4150-aab6-5188d1c23bdb
[source, python]
----
In [48]: xd = np.linspace(1.0, 3.0, 50)
         iyd = spi.splev(xd, ipo)
----

// code cell end

<<sin_plot_ipo_2>> reveals that the interpolation function indeed interpolates _linearly_ between two observation points. For certain applications this might not be precise enough. In addition, it is evident that the function is not continuously differentiable at the original data points--another drawback:

// code cell start uuid: 7ba1134b-ac71-4c3a-a0fc-b644c59effc6
[source, python]
----
In [49]: plt.plot(xd, f(xd), 'b', label='f(x)')
         plt.plot(xd, iyd, 'r.', label='interpolation')
         plt.legend(loc=0)
         plt.grid(True)
         plt.xlabel('x')
         plt.ylabel('f(x)')
----

[[sin_plot_ipo_2]]
.Example plot (detail) with linear interpolation
image::images/pyfi_0912.png[]

// code cell end

Therefore, let us repeat the complete exercise, this time using cubic splines:

// code cell start uuid: c3d9f85f-671f-4353-beab-fad9db9f8aa4
[source, python]
----
In [50]: ipo = spi.splrep(x, f(x), k=3)
         iyd = spi.splev(xd, ipo)
----

// code cell end

Now, the detailed subinterval in <<sin_plot_ipo_3>> shows a graphically perfect interpolation:

// code cell start uuid: 0c3e96f1-8732-4ce6-a6ed-bbaae732d950
[source, python]
----
In [51]: plt.plot(xd, f(xd), 'b', label='f(x)')
         plt.plot(xd, iyd, 'r.', label='interpolation')
         plt.legend(loc=0)
         plt.grid(True)
         plt.xlabel('x')
         plt.ylabel('f(x)')
----

[[sin_plot_ipo_3]]
.Example plot (detail) with cubic spline interpolation
image::images/pyfi_0913.png[]

// code cell end

Numerically, the interpolation is not perfect, but the MSE is really small:

// code cell start uuid: 8c67b514-160e-40e3-8f69-da81d7579b2d
[source, python]
----
In [52]: np.allclose(f(xd), iyd)
----

----
Out[52]: False
----

// code cell end

// code cell start uuid: ecc6c2fb-faa4-4149-b71e-05ac5cddeb79
[source, python]
----
In [53]: np.sum((f(xd) - iyd) ** 2) / len(xd)
----

----
Out[53]: 1.1349319851436252e-08
----

// code cell end

.Interpolation
[TIP]
====
(((range="endofrange", startref="ix_app")))(((range="endofrange", startref="ix_int")))In those cases where spline interpolation can be applied you can expect better approximation results compared to a least-squares regression approach. However, remember that you need to have sorted (and "nonnoisy") data and that the approach is limited to low-dimensional problems. It is also computationally more demanding and might therefore take (much) longer than regression in certain use cases.
====


=== Convex Optimization

((("mathematical tools", "convex optimization", id="ix_MTcon", range="startofrange")))((("convex optimization", id="ix_con", range="startofrange")))((("optimization", "convex", id="ix_Opcon", range="startofrange")))In finance and economics, _convex optimization_ plays an important role. Examples are the calibration of option pricing models to market data or the optimization of an agent's utility. As an example function that we want to minimize, we take +fm+, as defined in the following:

// code cell start uuid: a09a918e-760c-4e9c-95b8-fe3b568dacdb
[source, python]
----
In [54]: def fm((x, y)):
             return (np.sin(x) + 0.05 * x ** 2
                   + np.sin(y) + 0.05 * y ** 2)
----

// code cell end

// code cell start uuid: 1f4a147b-d9d2-42d5-8553-dd95d75d8251
[source, python]
----
In [55]: x = np.linspace(-10, 10, 50)
         y = np.linspace(-10, 10, 50)
         X, Y = np.meshgrid(x, y)
         Z = fm((X, Y))
----

// code cell end

<<opt_plot_3d>> shows the function graphically for the defined intervals for +x+ and +y+. Visual inspection already reveals that this function has multiple local minima. The existence of a global minimum cannot really be confirmed by this particular graphical pass:[<phrase role='keep-together'>representation:</phrase>]

// code cell start uuid: b3a1d3b7-25eb-4ac8-81ed-ad740fd1a297
[source, python]
----
In [56]: fig = plt.figure(figsize=(9, 6))
         ax = fig.gca(projection='3d')
         surf = ax.plot_surface(X, Y, Z, rstride=2, cstride=2, 
                 cmap=mpl.cm.coolwarm,
                 linewidth=0.5, antialiased=True)
         ax.set_xlabel('x')
         ax.set_ylabel('y')
         ax.set_zlabel('f(x, y)')
         fig.colorbar(surf, shrink=0.5, aspect=5)
----

[[opt_plot_3d]]
.Function to minimize with two parameters
image::images/pyfi_0914.png[]

// code cell end

(((SciPy, scipy.optimize sublibrary)))(((brute function)))(((fmin function)))In what follows, we want to implement both a _global_ minimization approach and a _local_ one. The functions +brute+ and +fmin+ that we want to use can be found in the sublibrary +scipy.optimize+:

// code cell start uuid: 72b2e44d-2127-412e-9046-2b0c7c5bf7c6
[source, python]
----
In [57]: import scipy.optimize as spo
----

// code cell end


==== Global Optimization

(((convex optimization, global)))(((global optimization)))(((optimization, global)))To have a closer look behind the scenes when we initiate the minimization procedures, we amend the original function by an option to output current parameter values as well as the function value:

// code cell start uuid: e6c829bb-f358-430d-9e8c-781e9d15c9e6
[source, python]
----
In [58]: def fo((x, y)):
             z = np.sin(x) + 0.05 * x ** 2 + np.sin(y) + 0.05 * y ** 2
             if output == True:
                 print '%8.4f %8.4f %8.4f' % (x, y, z)
             return z
----

// code cell end

This allows us to keep track of all relevant information for the procedure, as the following code with its respective output illustrates. +brute+ takes the parameter ranges as input. For example, providing parameter range +(-10, 10.1, 5)+ for the +x+ value will lead to "tested" values of +-10, -5, 0, 5, 10+:

// code cell start uuid: d1f54d32-6cb5-4cd2-8a95-ea3b92c87266
[source, python]
----
In [59]: output = True
         spo.brute(fo, ((-10, 10.1, 5), (-10, 10.1, 5)), finish=None)
----

----
Out[59]: -10.0000 -10.0000  11.0880
         -10.0000 -10.0000  11.0880
         -10.0000  -5.0000   7.7529
         -10.0000   0.0000   5.5440
         -10.0000   5.0000   5.8351
         -10.0000  10.0000  10.0000
          -5.0000 -10.0000   7.7529
          -5.0000  -5.0000   4.4178
          -5.0000   0.0000   2.2089
          -5.0000   5.0000   2.5000
          -5.0000  10.0000   6.6649
           0.0000 -10.0000   5.5440
           0.0000  -5.0000   2.2089
           0.0000   0.0000   0.0000
           0.0000   5.0000   0.2911
           0.0000  10.0000   4.4560
           5.0000 -10.0000   5.8351
           5.0000  -5.0000   2.5000
           5.0000   0.0000   0.2911
           5.0000   5.0000   0.5822
           5.0000  10.0000   4.7471
          10.0000 -10.0000  10.0000
          10.0000  -5.0000   6.6649
          10.0000   0.0000   4.4560
          10.0000   5.0000   4.7471
          10.0000  10.0000   8.9120
         
         array([ 0.,  0.])
----

// code cell end

The optimal parameter values, given the initial parameterization of the function, are ++x++ = ++y++ = 0. The resulting function value is also 0, as a quick review of the preceding output reveals. The first parameterization here is quite rough, in that we used steps of width 5 for both input parameters. This can of course be refined considerably, leading to better results in this case:

// code cell start uuid: dc42167b-4e8f-462c-8aad-a94afdf0a8bc
[source, python]
----
In [60]: output = False
         opt1 = spo.brute(fo, ((-10, 10.1, 0.1), (-10, 10.1, 0.1)), finish=None)
         opt1
----

----
Out[60]: array([-1.4, -1.4])
----

// code cell end

// code cell start uuid: ec2669cd-36a5-4b79-a90c-761a39b12044
[source, python]
----
In [61]: fm(opt1)
----

----
Out[61]: -1.7748994599769203
----

// code cell end

The optimal parameter values are now +x+ = +y+ = –1.4 and the minimal function value for the global minimization is about –1.7749.


==== Local Optimization

(((convex optimization, local)))(((local optimization)))(((optimization, local)))For the local convex optimization we want to draw on the results from the global optimization. The function +fmin+ takes as input the function to minimize and the starting parameter values. In addition, you can define levels for the input parameter tolerance and the function value tolerance, as well as for the maximum number of iterations and function calls:

// code cell start uuid: e451a278-e2ae-4d86-b357-f230468052a4
[source, python]
----
In [62]: output = True
         opt2 = spo.fmin(fo, opt1, xtol=0.001, ftol=0.001, maxiter=15, maxfun=20)
         opt2
----

----
Out[62]:  -1.4000  -1.4000  -1.7749
          -1.4700  -1.4000  -1.7743
          -1.4000  -1.4700  -1.7743
          -1.3300  -1.4700  -1.7696
          -1.4350  -1.4175  -1.7756
          -1.4350  -1.3475  -1.7722
          -1.4088  -1.4394  -1.7755
          -1.4438  -1.4569  -1.7751
          -1.4328  -1.4427  -1.7756
          -1.4591  -1.4208  -1.7752
          -1.4213  -1.4347  -1.7757
          -1.4235  -1.4096  -1.7755
          -1.4305  -1.4344  -1.7757
          -1.4168  -1.4516  -1.7753
          -1.4305  -1.4260  -1.7757
          -1.4396  -1.4257  -1.7756
          -1.4259  -1.4325  -1.7757
          -1.4259  -1.4241  -1.7757
          -1.4304  -1.4177  -1.7757
          -1.4270  -1.4288  -1.7757
         Warning: Maximum number of function evaluations has been exceeded.
         
         array([-1.42702972, -1.42876755])
----

// code cell end

Again, we can observe a refinement of the solution and a somewhat lower function value:

// code cell start uuid: 0352d556-6ea7-47a6-9c76-e4a56c0efed7
[source, python]
----
In [63]: fm(opt2)
----

----
Out[63]: -1.7757246992239009
----

// code cell end

For many convex optimization problems it is advisable to have a global minimization before the local one. The major reason for this is that local convex optimization algorithms can easily be trapped in a local minimum (or do "basin hopping"), ignoring completely "better" local minima and/or a global minimum. The following shows that setting the starting parameterization to ++x++ = ++y++ = 2 gives a "minimum" value of above zero:

// code cell start uuid: ff9beaf9-73d7-4157-85cb-24783abae775
[source, python]
----
In [64]: output = False
         spo.fmin(fo, (2.0, 2.0), maxiter=250)
----

----
Out[64]: Optimization terminated successfully.
                  Current function value: 0.015826
                  Iterations: 46
                  Function evaluations: 86
         
         array([ 4.2710728 ,  4.27106945])
----

// code cell end


==== Constrained Optimization

(((convex optimization, constrained)))(((constrained optimization)))(((optimization, constrained)))So far, we have only considered unconstrained optimization problems. However, large classes of economic or financial optimization problems are constrained by one or multiple constraints. Such constraints can formally take on the form of equations or pass:[<phrase role='keep-together'>inequalities.</phrase>]

As a simple example, consider the utility maximization problem of an (expected utility maximizing) investor who can invest in two risky securities. Both securities cost __q~a~__ = __q~b~__ = 10 today. After one year, they have a payoff of 15 USD and 5 USD, respectively, in state __u__, and of 5 USD and 12 USD, respectively, in state __d__. Both states are equally likely. Denote the vector payoffs for the two securities by __r~a~__ and __r~b~__, respectively.

The investor has a budget of __w~0~__ = 100 USD to invest and derives utility from future wealth according to the utility function latexmath:[$u(w) = \sqrt{w}$], where __w__ is the wealth (USD amount) available. <<u_max_1>> is a formulation of the maximization problem where __a__,__b__ are the numbers of securities bought by the investor.

[[u_max_1]]
[latexmath]
.Expected utility maximizing problem
++++
\begin{eqnarray*}
\max_{a,b} \mathbf{E}(u(w_1))&=& p \sqrt{w_{1u}} + (1-p) \sqrt{w_{1d}} \\
w_1 &=& a r_a + b r_b \\
w_0 &\geq& a q_a + b q_b \\
a, b &\geq& 0
\end{eqnarray*}
++++

Putting in all numerical assumptions, we get the problem in <<u_max_2>>. Note that we also change to the minimization of the negative expected utility.

[[u_max_2]]
[latexmath]
.Expected utility maximizing problem
++++
\begin{eqnarray*}
\min_{a,b} -\mathbf{E}(u(w_1))&=& -(0.5 \cdot\sqrt{w_{1u}} + 0.5 \cdot\sqrt{w_{1d}}) \\
w_{1u} &=& a \cdot 15 + b \cdot 5 \\
w_{1d} &=& a \cdot 5 + b \cdot 12 \\
100 &\geq& a \cdot 10 + b  \cdot 10 \\
a, b &\geq& 0
\end{eqnarray*}
++++

(((SciPy, scipy.optimize.minimize function)))To solve this problem, we use the +scipy.optimize.minimize+ function. This function takes as input--in addition to the function to be minimized--equations and inequalities (as a +list+ of +dict+ objects) as well as boundaries for the parameters (as a +tuple+ of +tuple+ objects).footnote:[For details and examples of how to use the +minimize+ function, refer to the http://bit.ly/using_minimize[documentation].] We can translate the problem from <<u_max_2>> into the following code:

// code cell start uuid: 703a8ac0-1c74-4851-9fd3-7ac404ee3446
[source, python]
----
In [65]: # function to be minimized
         from math import sqrt
         def Eu((s, b)):
             return -(0.5 * sqrt(s * 15 + b * 5) + 0.5 * sqrt(s * 5 + b * 12))
         
         # constraints
         cons = ({'type': 'ineq', 'fun': lambda (s, b):  100 - s * 10 - b * 10})
           # budget constraint
         bnds = ((0, 1000), (0, 1000))  # uppper bounds large enough
----

// code cell end

We have everything we need to use the +minimize+ function--we just have to add an initial guess for the optimal parameters:

// code cell start uuid: 8f989832-637a-4188-b096-26ceeabd77ea
[source, python]
----
In [66]: result = spo.minimize(Eu, [5, 5], method='SLSQP',
                                bounds=bnds, constraints=cons)
----

// code cell end

// code cell start uuid: 67c11995-124c-4f9d-8edf-1364b412e754
[source, python]
----
In [67]: result
----

----
Out[67]:   status: 0
          success: True
             njev: 5
             nfev: 21
              fun: -9.700883611487832
                x: array([ 8.02547122,  1.97452878])
          message: 'Optimization terminated successfully.'
              jac: array([-0.48508096, -0.48489535,  0.        ])
              nit: 5
----

// code cell end

The function returns a +dict+ object. The optimal parameters can be read out as follows:

// code cell start uuid: 53899a89-8532-4f22-a17f-54d106412867
[source, python]
----
In [68]: result['x']
----

----
Out[68]: array([ 8.02547122,  1.97452878])
----

// code cell end

The optimal function value is (changing the sign again):

// code cell start uuid: b8548809-fe3a-4fa4-9496-8d3b6a177973
[source, python]
----
In [69]: -result['fun']
----

----
Out[69]: 9.700883611487832
----

// code cell end

(((range="endofrange", startref="ix_MTcon")))(((range="endofrange", startref="ix_con")))(((range="endofrange", startref="ix_Opcon")))Given the parameterization for the simple model, it is optimal for the investor to buy about eight units of security __a__ and about two units of security __b__. The budget constraint is binding; i.e., the investor invests his/her total wealth of 100 USD into the securities. This is easily verified through taking the dot product of the optimal parameter vector and the price vector:

// code cell start uuid: 7453e87b-a70d-47b9-8a12-c5c0f01e4b86
[source, python]
----
In [70]: np.dot(result['x'], [10, 10])
----

----
Out[70]: 99.999999999999986
----

// code cell end


=== Integration

(((mathematical tools, integration)))(((SciPy, scipy.integrate sublibrary)))(((integration, scipy.integrate sublibrary)))Especially when it comes to valuation and option pricing, integration is an important mathematical tool. This stems from the fact that risk-neutral values of derivatives can be expressed in general as the discounted _expectation_ of their payoff under the risk-neutral (martingale) measure. The expectation in turn is a sum in the discrete case and an integral in the continuous case. The sublibrary +scipy.integrate+ provides different functions for numerical integration:

// code cell start uuid: 6a38567d-4a2d-464f-8de3-7687e1d2916d
[source, python]
----
In [71]: import scipy.integrate as sci
----

// code cell end

Again, we stick to the example function comprised of a +sin+ component and a pass:[<phrase role='keep-together'>linear one:</phrase>]

// code cell start uuid: 0954e5b6-9874-4bad-a2fb-28273a015e4a
[source, python]
----
In [72]: def f(x):
             return np.sin(x) + 0.5 * x
----

// code cell end

We are interested in the integral over the interval [0.5, 9.5]; i.e., the integral as in <<int_exam>>.

[[int_exam]]
[latexmath]
.Integral of example function
++++
\begin{equation*}
\int_{0.5}^{9.5} \sin(x) + 0.5 x dx
\end{equation*}
++++

<<sin_integral>> provides a graphical representation of the integral with a plot of the function __f__(__x__) &#x2261; sin(__x__) + 0.5__x__:

// code cell start uuid: afb6151f-5743-4950-980e-18047e066ebc
[source, python]
----
In [73]: a = 0.5  # left integral limit
         b = 9.5  # right integral limit
         x = np.linspace(0, 10)
         y = f(x)
----

// code cell end

// code cell start uuid: ad78923f-e4c1-4381-9a24-33b4f1ea3c56
[source, python]
----
In [74]: from matplotlib.patches import Polygon
         
         fig, ax = plt.subplots(figsize=(7, 5))
         plt.plot(x, y, 'b', linewidth=2)
         plt.ylim(ymin=0)
         
         # area under the function
         # between lower and upper limit
         Ix = np.linspace(a, b)
         Iy = f(Ix)
         verts = [(a, 0)] + list(zip(Ix, Iy)) + [(b, 0)]
         poly = Polygon(verts, facecolor='0.7', edgecolor='0.5')
         ax.add_patch(poly)
         
         # labels
         plt.text(0.75 * (a + b), 1.5, r"$\int_a^b f(x)dx$",
                  horizontalalignment='center', fontsize=20)
         
         plt.figtext(0.9, 0.075, '$x$')
         plt.figtext(0.075, 0.9, '$f(x)$')
         
         ax.set_xticks((a, b))
         ax.set_xticklabels(('$a$', '$b$'))
         ax.set_yticks([f(a), f(b)])
----

[[sin_integral]]
.Example function with integral area
image::images/pyfi_0915.png[]

// code cell end


==== Numerical Integration

(((integration, numerical)))(((Romberg integration)))(((fixed Gaussian quadrature)))(((Guassian quadrature)))(((adaptive quadrature)))((("quadratures, fixed Gaussian and adaptive")))(((integrate sublibrary)))The +integrate+ sublibrary contains a selection of functions to numerically integrate a given mathematical function given upper and lower integration limits. Examples are +fixed\_quad+ for _fixed Gaussian quadrature_, +quad+ for _adaptive quadrature_, and +romberg+ for _Romberg integration_:

// code cell start uuid: 6d3aa9c8-58c0-46a3-a89f-ca471a1f83e4
[source, python]
----
In [75]: sci.fixed_quad(f, a, b)[0]
----

----
Out[75]: 24.366995967084588
----

// code cell end

// code cell start uuid: 0ddebba6-5357-4ce9-b829-d752132d12df
[source, python]
----
In [76]: sci.quad(f, a, b)[0]
----

----
Out[76]: 24.374754718086752
----

// code cell end

// code cell start uuid: 3b1779d6-c831-449d-964f-e2cc869847e7
[source, python]
----
In [77]: sci.romberg(f, a, b)
----

----
Out[77]: 24.374754718086713
----

// code cell end

(((trapezoidal rule)))(((Simpson's rule)))There are also a number of integration functions that take as input +list+ or +ndarray+ objects with function values and input values. Examples in this regard are +trapz+, using the _trapezoidal_ rule, and +simps+, implementing _Simpson's_ rule:

// code cell start uuid: 5499809d-f2b1-4cc9-8808-511f4e82c5ed
[source, python]
----
In [78]: xi = np.linspace(0.5, 9.5, 25)
----

// code cell end

// code cell start uuid: aec239bc-cdd9-4477-b47c-4d0136ed686d
[source, python]
----
In [79]: sci.trapz(f(xi), xi)
----

----
Out[79]: 24.352733271544516
----

// code cell end

// code cell start uuid: ce33e420-5143-4d54-b23f-0040f54723fc
[source, python]
----
In [80]: sci.simps(f(xi), xi)
----

----
Out[80]: 24.374964184550748
----

// code cell end


==== Integration by Simulation

(((integration, by simulation)))(((simulation, numerical integration by)))(((Monte Carlo simulation, integration by simulation)))The valuation of options and derivatives by Monte Carlo simulation (cf. <<stochastics>>) rests on the insight that you can evaluate an integral by simulation. To this end, draw __I__ random values of __x__ between the integral limits and evaluate the integration function at every random value of __x__. Sum up all the function values and take the average to arrive at an average function value over the integration interval. Multiply this value by the length of the integration interval to derive an estimate for the integral value.

The following code shows how the Monte Carlo estimated integral value converges to the real one when one increases the number of random draws. The estimator is already quite close for really small numbers of random draws:

// code cell start uuid: bb840eb2-0e13-40eb-aea7-0f106dcf9f41
[source, python]
----
In [81]: for i in range(1, 20):
             np.random.seed(1000)
             x = np.random.random(i * 10) * (b - a) + a
             print np.sum(f(x)) / len(x) * (b - a)
----

----
Out[81]: 24.8047622793
         26.5229188983
         26.2655475192
         26.0277033994
         24.9995418144
         23.8818101416
         23.5279122748
         23.507857659
         23.6723674607
         23.6794104161
         24.4244017079
         24.2390053468
         24.115396925
         24.4241919876
         23.9249330805
         24.1948421203
         24.1173483782
         24.1006909297
         23.7690510985
         
----

// code cell end


=== Symbolic Computation

(((mathematical tools, symbolic computation)))The previous sections are mainly concerned with numerical computation. This section now introduces _symbolic_ computation, which can be applied beneficially in many areas of finance. To this end, let us import +SymPy+, the library specifically dedicated to symbolic computation:

// code cell start uuid: f01d358d-8094-426c-8e69-657218d8c319
[source, python]
----
In [82]: import sympy as sy
----

// code cell end


==== Basics

(((Symbol class)))(((symbolic computation, basics of)))(((SymPy library, Symbol class)))+SymPy+ introduces new classes of objects. A fundamental class is the +Symbol+ class:

// code cell start uuid: 2cb7d5d8-ae45-4001-863e-714df216a656
[source, python]
----
In [83]: x = sy.Symbol('x')
         y = sy.Symbol('y')
----

// code cell end

// code cell start uuid: 7bf55a76-f348-4820-af94-a83ed233d9a2
[source, python]
----
In [84]: type(x)
----

----
Out[84]: sympy.core.symbol.Symbol
----

// code cell end

(((SymPy library, mathematical function definitions)))Like +NumPy+, +SymPy+ has a number of (mathematical) function definitions. For example:

// code cell start uuid: a8f0388b-1df6-4389-877c-af5d9f141708
[source, python]
----
In [85]: sy.sqrt(x)
----

----
Out[85]: sqrt(x)
----

// code cell end

This already illustrates a major difference. Although +x+ has no numerical value, the square root of +x+ is nevertheless defined with +SymPy+ since +x+ is a +Symbol+ object. In that sense, +sy.sqrt(x)+ can be part of arbitrary mathematical expressions. Notice that +SymPy+ in general automatically simplifies a given mathematical expression:

// code cell start uuid: f308fe2e-a5bc-4c88-a7f6-a4f09a568f2f
[source, python]
----
In [86]: 3 + sy.sqrt(x) - 4 ** 2
----

----
Out[86]: sqrt(x) - 13
----

// code cell end

Similarly, you can define arbitrary functions using +Symbol+ objects. They are not to be confused with +Python+ functions:

// code cell start uuid: 025f5084-4efc-4416-a564-b8a96ec426fd
[source, python]
----
In [87]: f = x ** 2 + 3 + 0.5 * x ** 2 + 3 / 2
----

// code cell end

// code cell start uuid: f427581e-50ba-45b9-a145-ff734be26e59
[source, python]
----
In [88]: sy.simplify(f)
----

----
Out[88]: 1.5*x**2 + 4
----

// code cell end

+SymPy+ provides three basic renderers for mathematical expressions:

* +LaTeX+-based
* +Unicode+-based
* +ASCII+-based

When working, for example, solely in the +IPython+ +Notebook+, +LaTeX+ rendering is generally a good (i.e., visually appealing) choice. In what follows, we stick to the simplest option, +ASCII+, to illustrate that there is no handmade type setting involved:

// code cell start uuid: 59837e7c-a82f-44e5-8b87-b0fba310ffc5
[source, python]
----
In [89]: sy.init_printing(pretty_print=False, use_unicode=False)
----

// code cell end

// code cell start uuid: ea1bbb85-a431-4e6c-a264-6ffb6df9f59e
[source, python]
----
In [90]: print sy.pretty(f)
----

----
Out[90]:      2    
         1.5*x  + 4
         
----

// code cell end

As you can see from the output, multiple lines are used whenever needed. Also, for example, see the following for the visual representation of the square-root function:

// code cell start uuid: e8702014-5347-4eac-b10c-39c5a54459c9
[source, python]
----
In [91]: print sy.pretty(sy.sqrt(x) + 0.5)
----

----
Out[91]:   ___      
         \/ x  + 0.5
         
----

// code cell end

We can not go into details here, but +SymPy+ also provides many other useful mathematical functions--for example, when it comes to numerically evaluating &#x1d70b;. The following shows the first 40 characters of the  +string+ representation of &#x1d70b; up to the 400,000th digit:

// code cell start uuid: b8cf7cf4-56e2-4c6b-a70b-6592ec4bd6a6
[source, python]
----
In [92]: pi_str = str(sy.N(sy.pi, 400000))
         pi_str[:40]
----

----
Out[92]: '3.14159265358979323846264338327950288419'
----

// code cell end

And here are the last 40 digits of the first 400,000:

// code cell start uuid: 595cc15e-99b8-43c7-ad89-db4500fdb860
[source, python]
----
In [93]: pi_str[-40:]
----

----
Out[93]: '8245672736856312185020980470362464176198'
----

// code cell end

You can also look up your birthday if you wish; however, there is no guarantee of a hit:

// code cell start uuid: cc4b85c4-e603-4d36-9553-f3999e877fca
[source, python]
----
In [94]: pi_str.find('111272')
----

----
Out[94]: 366713
----

// code cell end


==== Equations

(((symbolic computation, equations)))(((SymPy library, equation solving with)))A strength of +SymPy+ is solving equations, e.g., of the form __x__^2^ – 1 = 0:

// code cell start uuid: c310eeab-ac04-4015-a327-29628f9a9a11
[source, python]
----
In [95]: sy.solve(x ** 2 - 1)
----

----
Out[95]: [-1, 1]
----

// code cell end

In general, +SymPy+ presumes that you are looking for a solution to the equation obtained by equating the given expression to zero. Therefore, equations like __x__^2^ – 1 = 3 might have to be reformulated to get the desired result:

// code cell start uuid: 60076f9d-93f3-47e0-b5b6-87d1bca3d30a
[source, python]
----
In [96]: sy.solve(x ** 2 - 1 - 3)
----

----
Out[96]: [-2, 2]
----

// code cell end

Of course, +SymPy+ can cope with more complex expressions, like __x__^3^ + 0.5__x__^2^ – 1 = 0:

// code cell start uuid: 6c9e1060-61e6-4b18-9be4-cb71b83fc5a7
[source, python]
----
In [97]: sy.solve(x ** 3 + 0.5 * x ** 2 - 1)
----

----
Out[97]: [0.858094329496553, -0.679047164748276 - 0.839206763026694*I,
         -0.679047164748276 + 0.839206763026694*I]
----

// code cell end

However, there is obviously no guarantee of a solution, either from a mathematical point of view (i.e., the existence of a solution) or from an algorithmic point of view (i.e., an implementation).

+SymPy+ works similarly with functions exhibiting more than one input parameter, and to this end also with complex numbers. As a simple example take the equation __x__^2^ + __y__^2^ = 0:

// code cell start uuid: fcd70615-0db5-4c5c-8246-245ba3c8501c
[source, python]
----
In [98]: sy.solve(x ** 2 + y ** 2)
----

----
Out[98]: [{x: -I*y}, {x: I*y}]
----

// code cell end


==== Integration

(((symbolic computation, integration)))(((integration, symbolic computation)))(((SymPy library, integration with)))Another strength of +SymPy+ is integration and differentiation. In what follows, we revisit the example function used for numerical- and simulation-based integration and derive now both a _symbolic_ and a _numerically_ exact solution. We need symbols for the integration limits:

// code cell start uuid: 503329d1-812a-461c-9019-e6bb2a44125e
[source, python]
----
In [99]: a, b = sy.symbols('a b')
----

// code cell end

Having defined the new symbols, we can "pretty print" the symbolic integral:

// code cell start uuid: af7bf980-d65b-4567-8576-404e79c5ac65
[source, python]
----
In [100]: print sy.pretty(sy.Integral(sy.sin(x) + 0.5 * x, (x, a, b)))
----

----
Out[100]:   b                    
            /                    
           |                     
           |  (0.5*x + sin(x)) dx
           |                     
          /                      
          a                      
          
----

// code cell end

Using +integrate+, we can then derive the _antiderivative_ of the integration function:

// code cell start uuid: 89d4c980-01d7-423a-a2ab-a9698787019a
[source, python]
----
In [101]: int_func = sy.integrate(sy.sin(x) + 0.5 * x, x)
----

// code cell end

// code cell start uuid: 38141163-7d29-4bd6-bfaf-9f9cfad86967
[source, python]
----
In [102]: print sy.pretty(int_func)
----

----
Out[102]:       2         
          0.25*x  - cos(x)
          
----

// code cell end

Equipped with the antiderivative, the numerical evaluation of the integral is only three steps away. To numerically evaluate a +SymPy+ expression, replace the respective symbol with the numerical value using the method +subs+ and call the method +evalf+ on the new expression:

// code cell start uuid: 51d4481a-c6b6-4caa-b4d1-aa6fb04d4493
[source, python]
----
In [103]: Fb = int_func.subs(x, 9.5).evalf()
          Fa = int_func.subs(x, 0.5).evalf()
----

// code cell end

The difference between +Fb+ and +Fa+ then yields the exact integral value:

// code cell start uuid: 0c46e097-c665-440e-9d05-03c0d6e6d8d8
[source, python]
----
In [104]: Fb - Fa  # exact value of integral
----

----
Out[104]: 24.3747547180867
----

// code cell end

The integral can also be solved symbolically with the symbolic integration limits:

// code cell start uuid: 657b13dc-101e-4c0c-9d15-43dbd418c3c8
[source, python]
----
In [105]: int_func_limits = sy.integrate(sy.sin(x) + 0.5 * x, (x, a, b))
          print sy.pretty(int_func_limits)
----

----
Out[105]:         2         2                  
          - 0.25*a  + 0.25*b  + cos(a) - cos(b)
          
----

// code cell end

As before, numerical substitution--this time using a +dict+ object for multiple substitutions--and evaluation then yields the integral value:

// code cell start uuid: 5c65cea8-1e15-4efa-aba4-52888becaf19
[source, python]
----
In [106]: int_func_limits.subs({a : 0.5, b : 9.5}).evalf()
----

----
Out[106]: 24.3747547180868
----

// code cell end

Finally, providing quantified integration limits yields the exact value in a single step:

// code cell start uuid: b13767c8-4878-42c1-b738-0727b824b329
[source, python]
----
In [107]: sy.integrate(sy.sin(x) + 0.5 * x, (x, 0.5, 9.5))
----

----
Out[107]: 24.3747547180867
----

// code cell end


==== Differentiation

(((symbolic computation, differentiation)))(((differentiation)))(((SymPy library, differentiation with)))The derivative of the antiderivative shall yield in general the original function. Let us check this by applying the +diff+ function to the symbolic antiderivative from before:

// code cell start uuid: 6ea93bb3-eb7c-4837-8327-621f3752b657
[source, python]
----
In [108]: int_func.diff()
----

----
Out[108]: 0.5*x + sin(x)
----

// code cell end

As with the integration example, we want to use differentiation now to derive the exact solution of the convex minimization problem we looked at earlier. To this end, we define the respective function symbolically as follows:

// code cell start uuid: 164131fd-a197-40ce-85d6-94221e808073
[source, python]
----
In [109]: f = (sy.sin(x) + 0.05 * x ** 2
             + sy.sin(y) + 0.05 * y ** 2)
----

// code cell end

For the minimization, we need the two partial derivatives with respect to both variables, ++x++ and ++y++:

// code cell start uuid: 5d1ce51b-2536-469b-b516-56db1e86519a
[source, python]
----
In [110]: del_x = sy.diff(f, x)
          del_x
----

----
Out[110]: 0.1*x + cos(x)
----

// code cell end

// code cell start uuid: 8d778c21-ed6a-43f0-8347-b95af334972a
[source, python]
----
In [111]: del_y = sy.diff(f, y)
          del_y
----

----
Out[111]: 0.1*y + cos(y)
----

// code cell end

A necessary but not sufficient condition for a global minimum is that both partial derivatives are zero. As stated before, there is no guarantee of a symbolic solution.
Both algorithmic and (multiple) existence issues come into play here. However, we can solve the two equations numerically, providing "educated" guesses based on the global and local minimization efforts from before:

// code cell start uuid: 588daaba-266a-43da-a4b9-baeb56fc1ac8
[source, python]
----
In [112]: xo = sy.nsolve(del_x, -1.5)
          xo
----

----
Out[112]: mpf('-1.4275517787645941')
----

// code cell end

// code cell start uuid: 2ef9beca-58f2-470b-a50e-6292bffbf2e0
[source, python]
----
In [113]: yo = sy.nsolve(del_y, -1.5)
          yo
----

----
Out[113]: mpf('-1.4275517787645941')
----

// code cell end

// code cell start uuid: c0812b29-c69f-4a30-aaf1-1447d1434cf4
[source, python]
----
In [114]: f.subs({x : xo, y : yo}).evalf()
            # global minimum
----

----
Out[114]: -1.77572565314742
----

// code cell end

Again, providing uneducated/arbitrary guesses might trap the algorithm in a local minimum instead of the global one:

// code cell start uuid: fd89d904-eab3-4035-941a-785c6a3f36cb
[source, python]
----
In [115]: xo = sy.nsolve(del_x, 1.5)
          xo
----

----
Out[115]: mpf('1.7463292822528528')
----

// code cell end

// code cell start uuid: cc99d8f4-e062-4c7e-85ae-46e96d3c8f19
[source, python]
----
In [116]: yo = sy.nsolve(del_y, 1.5)
          yo
----

----
Out[116]: mpf('1.7463292822528528')
----

// code cell end

// code cell start uuid: 54622abe-cef1-47ff-bf23-b883cf588a91
[source, python]
----
In [117]: f.subs({x : xo, y : yo}).evalf()
            # local minimum
----

----
Out[117]: 2.27423381055640
----

// code cell end

(((range="endofrange", startref="ix_Fmt")))(((range="endofrange", startref="ix_MTapp")))This numerically illustrates that zero partial derivatives are necessary but not sufficient.

.Symbolic Computations
[TIP]
====
(((SymPy library, benefits for symbolic computations)))When doing mathematics with +Python+, you should always think of +SymPy+ and symbolic computations. Especially for interactive financial analytics, this can be a more efficient approach compared to non-symbolic approaches.
====


=== Conclusions

This chapter covers some mathematical topics and tools important to finance. For example, the _approximation of functions_ is important in many financial areas, like yield curve interpolation and regression-based Monte Carlo valuation approaches for American options. _Convex optimization_ techniques are also regularly needed in finance; for example, when calibrating parametric option pricing models to market quotes or implied volatilities of options.

_Numerical integration_ is, for example, central to the pricing of options and derivatives. Having derived the risk-neutral probability measure for a (set of) stochastic process(es), option pricing boils down to taking the expectation of the option's payoff under the risk-neutral measure and discounting this value back to the present date. <<stochastics>> covers the simulation of several types of stochastic processes under the risk-neutral measure.

Finally, this chapter introduces _symbolic computation_ with +SymPy+. For a number of mathematical operations, like integration, differentiation, or the solving of equations, symbolic computation can prove a really useful and efficient tool.


=== Further Reading

For further information on the +Python+ libraries used in this chapter, you should consult the following web resources:

* See http://docs.scipy.org/doc/numpy/reference/[] for all functions used from +NumPy+.
* The +statsmodels+ library is documented here: http://statsmodels.sourceforge.net[].
* Visit http://docs.scipy.org/doc/scipy/reference/optimize.html[] for details on +scipy.optimize+.
* Integration with +scipy.integrate+ is explained here: http://docs.scipy.org/doc/scipy/reference/integrate.html[].
* The home of +SymPy+ is http://sympy.org[].

For a good reference to the mathematical topics covered, see: 

* Brandimarte, Paolo (2006): _Numerical Methods in Finance and Economics,_ 2nd ed. John Wiley & Sons, Hoboken, NJ.